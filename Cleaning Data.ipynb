{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "\n",
    "#### Student Name: VIDHU SHEKHAR TRIPATHI\n",
    "\n",
    "\n",
    "Date: 31/01/2021\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3.7.3 and Jupyter notebook\n",
    "\n",
    "    \n",
    "## Table of Contents\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "* 1. Task 1 DATA CLEANING\n",
    "\n",
    "    * 1.1. Import the libraries \n",
    "    \n",
    "    * 1.2. Reading the files \n",
    "    \n",
    "    * 1.3. Identify and fix missing data  \n",
    "    \n",
    "    * 1.4. Identify and fix data with wrong format \n",
    "    \n",
    "    * 1.5. Create a data model for each sector\n",
    "        * 1.5.1 IT\n",
    "        * 1.5.2 Finance\n",
    "        * 1.5.3 Data Science\n",
    "        \n",
    "    * 1.6. Making predictions to fix the remaining errors \n",
    "        * 1.6.1 Fixing the Missing Gender for Data Science\n",
    "        * 1.6.2 Fixing the Missing Gender for IT\n",
    "        * 1.6.3 Fixing the Missing Gender for Finance\n",
    "        * 1.6.4 Fixing the Missing Salary for Data Science\n",
    "        * 1.6.5 Fixing the Missing Salary for IT\n",
    "        * 1.6.6 Fixing the Missing Salary for Finance\n",
    "        * 1.6.7 Predicting the missing Sectors \n",
    "        \n",
    "     * 1.7 Save the File\n",
    "        \n",
    "* 2. Task 2 PARSING AND PRE PROCESSING OF TEXTUAL DATA\n",
    "\n",
    "    * 2.1. Reading the files \n",
    "    * 2.2. Create data set with ID and Tweets\n",
    "    * 2.3. Save the tweet file \n",
    "    * 2.4. Add Sentiment to each tweet in the dataframe\n",
    "    * 2.5. Create unigram tokens\n",
    "    * 2.6. Save the file \n",
    "        \n",
    "* 3. Task 3 DATA INTEGRATION \n",
    "    \n",
    "    * 3.1 Reading the files\n",
    "    * 3.2 Adding columns to existing dataframe\n",
    "        \n",
    "        * 3.2.1  Adding id and distance for Shopping centres\n",
    "        * 3.2.2  Adding id and distance for Stations\n",
    "        * 3.3.3  Adding average time to travel to CBD\n",
    "        \n",
    "    * 3.3 Save the file\n",
    "\n",
    "    \n",
    "* 4. Conclusion\n",
    "* 5. References\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<br>\n",
    "    \n",
    "## Task 1 - DATA CLEANING <a class=\"anchor\" name=\"task1\">\n",
    "    \n",
    "<br>\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import all the Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabula-py in c:\\users\\vidhu\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: distro in c:\\users\\vidhu\\anaconda3\\lib\\site-packages (from tabula-py) (1.5.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\vidhu\\anaconda3\\lib\\site-packages (from tabula-py) (1.18.5)\n",
      "Requirement already satisfied: pandas>=0.25.3 in c:\\users\\vidhu\\anaconda3\\lib\\site-packages (from tabula-py) (1.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\vidhu\\anaconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\vidhu\\anaconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vidhu\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas>=0.25.3->tabula-py) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "# Importing built in libraries and importing other libraries\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import os, re, langid, timeit\n",
    "import datetime\n",
    "from datetime import datetime,date\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "!pip install tabula-py\n",
    "import tabula\n",
    "import os\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "from math import radians,cos,sin,asin,sqrt \n",
    "from collections import Counter \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Reading files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the csv file\n",
    "dirty_data=pd.read_csv('dirty_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Identify and fix missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title', 'Gender', 'Sector', 'Salary']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the missing values\n",
    "missing_columns=dirty_data.columns[dirty_data.isnull().any()].tolist()\n",
    "missing_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty dataframes and list for future models\n",
    "\n",
    "# Since one row can have only one error , once a row is fixed its added to list of indexes required\n",
    "# to create the data model \n",
    "\n",
    "data_for_model=pd.DataFrame()\n",
    "list_of_index=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing Gender values 33\n",
      "------------------\n",
      "Number of missing Gender values 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-a7085c1ff971>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dirty_data['Gender'][i]=0.0\n",
      "<ipython-input-5-a7085c1ff971>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_data['Gender'][i]=0.0\n",
      "C:\\Users\\vidhu\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3343: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "<ipython-input-5-a7085c1ff971>:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dirty_data['Gender'][i]=1.0\n",
      "<ipython-input-5-a7085c1ff971>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_data['Gender'][i]=1.0\n"
     ]
    }
   ],
   "source": [
    "# Identifying and fixing missing values of Gender using Title values and Education \n",
    "# Note Title doctor is given to a person with education as 1 but its hard to identify Gender with Title as Doctor \n",
    "\n",
    "missing_data=dirty_data[dirty_data['Gender'].isnull()]\n",
    "print('Number of missing Gender values',len(missing_data))\n",
    "\n",
    "# Iterating over the values and fixing values using above mentioned values as point of comparison\n",
    "\n",
    "for i in missing_data['Unnamed: 0']:\n",
    "    if dirty_data['Title'][i]=='Mr':\n",
    "        dirty_data['Gender'][i]=0.0\n",
    "        missing_data['Gender'][i]=0.0\n",
    "        list_of_index.append(dirty_data['Unnamed: 0'][i])\n",
    "        \n",
    "    elif missing_data['Title'][i]=='Mrs':\n",
    "        dirty_data['Gender'][i]=1.0\n",
    "        missing_data['Gender'][i]=1.0\n",
    "        list_of_index.append(dirty_data['Unnamed: 0'][i])\n",
    "\n",
    "missing_data=missing_data[missing_data['Gender'].isnull()]\n",
    "print('------------------')\n",
    "print('Number of missing Gender values',len(missing_data))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing Title values 33\n",
      "------------------\n",
      "Number of missing Title values 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-1ab70b548db0>:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dirty_data['Title'][i]='Mr'\n",
      "<ipython-input-6-1ab70b548db0>:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_data2['Title'][i]='Mr'\n",
      "<ipython-input-6-1ab70b548db0>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dirty_data['Title'][i]='Dr'\n",
      "<ipython-input-6-1ab70b548db0>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_data2['Title'][i]='Dr'\n",
      "<ipython-input-6-1ab70b548db0>:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dirty_data['Title'][i]='Mrs'\n",
      "<ipython-input-6-1ab70b548db0>:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_data2['Title'][i]='Mrs'\n"
     ]
    }
   ],
   "source": [
    "# Identifying and fixing missing values of Title using Education and Gender values\n",
    "\n",
    "missing_data2=dirty_data[dirty_data['Title'].isnull()]\n",
    "print('Number of missing Title values',len(missing_data2))\n",
    "\n",
    "# Iterating over the values and fixing values using above mentioned values as point of comparison\n",
    "\n",
    "\n",
    "for i in missing_data2['Unnamed: 0']:\n",
    "    if missing_data2['Education'][i]==1:\n",
    "        dirty_data['Title'][i]='Dr'\n",
    "        missing_data2['Title'][i]='Dr'\n",
    "        list_of_index.append(dirty_data['Unnamed: 0'][i])\n",
    "        \n",
    "    elif missing_data2['Education'][i]!=1:\n",
    "        \n",
    "        if missing_data2['Gender'][i]==0.0:\n",
    "            dirty_data['Title'][i]='Mr'\n",
    "            missing_data2['Title'][i]='Mr'\n",
    "            list_of_index.append(dirty_data['Unnamed: 0'][i])\n",
    "            \n",
    "        elif missing_data2['Gender'][i]==1.0:\n",
    "            dirty_data['Title'][i]='Mrs'\n",
    "            missing_data2['Title'][i]='Mrs'\n",
    "            list_of_index.append(dirty_data['Unnamed: 0'][i])\n",
    "\n",
    "\n",
    "missing_data2=missing_data2[missing_data2['Gender'].isnull()]\n",
    "print('------------------')\n",
    "print('Number of missing Title values',len(missing_data2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Identify and fix data with wrong format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mismatched Gender and Titles 32\n"
     ]
    }
   ],
   "source": [
    "# Finding Mismatch data\n",
    "\n",
    "# 1. Gender and Title \n",
    "id4=[]\n",
    "for i in (dirty_data['Unnamed: 0']):\n",
    "    if (dirty_data['Gender'][i]==0) & (dirty_data['Title'][i]=='Mrs'):\n",
    "        id4.append(dirty_data['ID'][i])\n",
    "    elif (dirty_data['Gender'][i]==1) & (dirty_data['Title'][i]=='Mr'):\n",
    "        id4.append(dirty_data['ID'][i])\n",
    "print('Number of mismatched Gender and Titles',len(id4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-17f93f404d97>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dirty_data['Sector'][i]='IT'\n",
      "<ipython-input-8-17f93f404d97>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dirty_data['Sector'][i]='Finance'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Data Science', 'Finance', 'IT', nan}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Fixing spelling mistake\n",
    "\n",
    "# Nan will be fixed via model \n",
    "\n",
    "\n",
    "for i in dirty_data['Unnamed: 0']:\n",
    "    if dirty_data['Sector'][i]=='iT':\n",
    "        dirty_data['Sector'][i]='IT'\n",
    "        list_of_index.append(dirty_data['Unnamed: 0'][i])\n",
    "        \n",
    "    elif dirty_data['Sector'][i]=='Financ':\n",
    "        dirty_data['Sector'][i]='Finance'\n",
    "        list_of_index.append(dirty_data['Unnamed: 0'][i])\n",
    "\n",
    "\n",
    "set(dirty_data['Sector'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of defected formats of dates  50\n"
     ]
    }
   ],
   "source": [
    "#Dates \n",
    "A=[]\n",
    "# Create a function to check if the dates are in correct format \n",
    "def validate(date_text):\n",
    "    try:\n",
    "        datetime.strptime(date_text, '%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        A.append(date_text)\n",
    "# Find the values that are not in correct format    \n",
    "for i in range(len(dirty_data['Last_employment_date'])):\n",
    "    validate(dirty_data['Last_employment_date'][i])\n",
    "    \n",
    "# Print the total number of errors detected \n",
    "print(\"\\n Number of defected formats of dates \", len(A))\n",
    "# Total_errors_dirty_data=Total_errors_dirty_data+len(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-8a4cc6280ff5>:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dirty_data['Last_employment_date'][j]= str(C[i])\n"
     ]
    }
   ],
   "source": [
    "# 3. Fixing interchanged values\n",
    "B=[]\n",
    "C=[]\n",
    "# Disassemble the incorrect values \n",
    "for dates in A:\n",
    "    dates_temp=dates.split('-')\n",
    "    [int(x) for x in dates_temp]\n",
    "    B.append(dates_temp)\n",
    "for i in range(len(B)):  \n",
    "    B[i][0]=int(B[i][0])\n",
    "    B[i][1]=int(B[i][1])\n",
    "    B[i][2]=int(B[i][2])\n",
    "    # Using swap method to place values in correct format \n",
    "    if B[i][0] >2000: \n",
    "        if B[i][1]>12:\n",
    "            temp=0\n",
    "            temp=B[i][2]\n",
    "            B[i][2]=B[i][1]\n",
    "            B[i][1]=temp\n",
    "    if B[i][1]==2:\n",
    "        if B[i][2] > 28:\n",
    "            B[i][2] =28\n",
    "    \n",
    "    if B[i][1] in [4,6,8,10]:\n",
    "        if B[i][2] > 30:\n",
    "            B[i][2] =30\n",
    "    \n",
    "            \n",
    "# Re assemble the corrected values above in the required values\n",
    "for values in B:\n",
    "    C.append(str(values[0])+\"-\"+str(values[1])+\"-\"+str(values[2]))\n",
    "\n",
    "Dates=dirty_data['Last_employment_date']\n",
    "\n",
    "# Applying the corrective changes in dirty data\n",
    "\n",
    "for i in range(len(A)):\n",
    "    for j in range(len(Dates)):\n",
    "        if A[i] == dirty_data['Last_employment_date'][j]:\n",
    "            dirty_data['Last_employment_date'][j]= str(C[i])\n",
    "            list_of_index.append(dirty_data['Unnamed: 0'][j])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of defected formats of dates  0\n"
     ]
    }
   ],
   "source": [
    "#Performing the validation again\n",
    "A=[]\n",
    "C=[]\n",
    "\n",
    "# Create a function to check if the dates are in correct format \n",
    "\n",
    "def validate(date_text):\n",
    "    try:\n",
    "        datetime.strptime(date_text, '%Y-%m-%d')\n",
    "        \n",
    "    except ValueError:\n",
    "        print(date_text)\n",
    "        A.append(date_text)\n",
    "    \n",
    "for value in dirty_data['Last_employment_date']:\n",
    "    validate(value)\n",
    "\n",
    "print(\"\\n Number of defected formats of dates \", len(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-d6481d06b1e6>:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dirty_data['Latlng'][i]=string\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors in latitude and longitude 33\n"
     ]
    }
   ],
   "source": [
    "latlang=[]\n",
    "\n",
    "# 4. Finding and fixing Latlng column by swapping the values at wrong place and correcting their format \n",
    "for i in range(len(dirty_data['Latlng'])):\n",
    "    # Firstly we convert the string with breackets to two sepearate numbers to perform analysis\n",
    "    string=dirty_data['Latlng'][i]\n",
    "    string=string.replace('[','')\n",
    "    string=string.replace(']','')\n",
    "    string=string.split(' ')\n",
    "    string=' '.join(string).split()\n",
    "    x1=float(string[0])\n",
    "    x2=float(string[1])    \n",
    "    \n",
    "    # Validating both the derived numerical values that if they are at their correct place or not\n",
    "    # If not they are swapped \n",
    "    # If positive values are negative , their signs are changed \n",
    "    if -40 <= x1 <= -35 :\n",
    "        if x2 < 140 :\n",
    "            x2=x2*(-1)\n",
    "            list_of_index.append(dirty_data['Unnamed: 0'][i])\n",
    "            latlang.append(i)\n",
    "    if 140 <= x2 <= 150 :\n",
    "        if x1 > 30:\n",
    "            x1=x1*(-1)\n",
    "            list_of_index.append(dirty_data['Unnamed: 0'][i])\n",
    "            latlang.append(i)\n",
    "    if (140 <= x1 <=150) & (-40 <= x2 <= -35) :\n",
    "        x1,x2=x2,x1 \n",
    "        list_of_index.append(dirty_data['Unnamed: 0'][i])\n",
    "        latlang.append(i)\n",
    "        \n",
    "    string=str('[')+str(x1)+str(' ')+str(x2)+str(']')\n",
    "    dirty_data['Latlng'][i]=string\n",
    "\n",
    "print('Errors in latitude and longitude',len(latlang))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Create a data model for each sector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-08e66d902c3a>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  model_dataset.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Creating model dataset from the collected list \n",
    "model_dataset=dirty_data[dirty_data['Unnamed: 0'].isin(list_of_index)]\n",
    "print(len(model_dataset['Unnamed: 0']))\n",
    "model_dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "61\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "# creating model dataset for each sector\n",
    "model_dataset_IT=model_dataset[model_dataset['Sector']=='IT']\n",
    "model_dataset_finance=model_dataset[model_dataset['Sector']=='Finance']\n",
    "model_dataset_DS=model_dataset[model_dataset['Sector']=='Data Science']\n",
    "\n",
    "\n",
    "\n",
    "print(len(model_dataset_IT))\n",
    "print(len(model_dataset_finance))\n",
    "print(len(model_dataset_DS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data for each sector \n",
    "dirty_data_it=dirty_data[dirty_data['Sector']=='IT']\n",
    "dirty_data_fin=dirty_data[dirty_data['Sector']=='Finance']\n",
    "dirty_data_DS=dirty_data[dirty_data['Sector']=='Data Science']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1  IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X and Y values for IT\n",
    "X1=model_dataset_IT[['Education','Age','Work_experience','Gender']]\n",
    "Y1=model_dataset_IT['Salary']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into test and train\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, Y1, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the regression model \n",
    "regressor1 = LinearRegression()\n",
    "regressor1.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making prediction\n",
    "coeff_df = pd.DataFrame(regressor1.coef_, X1.columns, columns=['Coefficient'])\n",
    "y_pred1 = regressor1.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>1600.0</td>\n",
       "      <td>1600.342202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>1717.0</td>\n",
       "      <td>1715.727576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>1021.0</td>\n",
       "      <td>1018.732828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>1515.0</td>\n",
       "      <td>1513.050805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>1014.0</td>\n",
       "      <td>1012.556624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2189</th>\n",
       "      <td>2624.0</td>\n",
       "      <td>2619.474243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>814.0</td>\n",
       "      <td>811.980909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>4970.0</td>\n",
       "      <td>4968.732675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>6647.0</td>\n",
       "      <td>6648.789281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3076</th>\n",
       "      <td>1513.0</td>\n",
       "      <td>1516.138908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2258</th>\n",
       "      <td>1321.0</td>\n",
       "      <td>1321.739397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1632</th>\n",
       "      <td>2799.0</td>\n",
       "      <td>2798.433241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1673</th>\n",
       "      <td>3640.0</td>\n",
       "      <td>3633.940600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>2208.0</td>\n",
       "      <td>2204.170403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>811.0</td>\n",
       "      <td>808.892806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual    Predicted\n",
       "1623  1600.0  1600.342202\n",
       "1630  1717.0  1715.727576\n",
       "2157  1021.0  1018.732828\n",
       "1561  1515.0  1513.050805\n",
       "1663  1014.0  1012.556624\n",
       "2189  2624.0  2619.474243\n",
       "158    814.0   811.980909\n",
       "2294  4970.0  4968.732675\n",
       "1723  6647.0  6648.789281\n",
       "3076  1513.0  1516.138908\n",
       "2258  1321.0  1321.739397\n",
       "1632  2799.0  2798.433241\n",
       "1673  3640.0  3633.940600\n",
       "1994  2208.0  2204.170403\n",
       "1701   811.0   808.892806"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparing Actual vs Predicted\n",
    "df1 = pd.DataFrame({'Actual': y_test1, 'Predicted': y_pred1})\n",
    "df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 2.2211385761930083\n",
      "Mean Squared Error: 7.244589908453082\n",
      "Root Mean Squared Error: 2.691577587299516\n"
     ]
    }
   ],
   "source": [
    "# Statistically analyzing model performance\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test1, y_pred1))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test1, y_pred1))\n",
    "print('Root Mean Squared Error:', math.sqrt(metrics.mean_squared_error(y_test1, y_pred1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 Finance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X and Y values for Finance\n",
    "\n",
    "X2=model_dataset_finance[['Education','Age','Work_experience','Gender']]\n",
    "Y2=model_dataset_finance['Salary']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into test and train\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, Y2, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the regression model \n",
    "\n",
    "regressor2 = LinearRegression()\n",
    "regressor2.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making prediction\n",
    "\n",
    "coeff_df2 = pd.DataFrame(regressor2.coef_, X2.columns, columns=['Coefficient'])\n",
    "y_pred2 = regressor2.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>3384.0</td>\n",
       "      <td>3382.402704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>2046.0</td>\n",
       "      <td>2042.917563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3075</th>\n",
       "      <td>2744.0</td>\n",
       "      <td>2747.137070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1676</th>\n",
       "      <td>1334.0</td>\n",
       "      <td>1335.875165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>4401.0</td>\n",
       "      <td>4400.895905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>3089.0</td>\n",
       "      <td>3083.152282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>1682.0</td>\n",
       "      <td>1683.184331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3074</th>\n",
       "      <td>1680.0</td>\n",
       "      <td>1681.772886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>2037.0</td>\n",
       "      <td>2035.011556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>993.0</td>\n",
       "      <td>995.906950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>5520.0</td>\n",
       "      <td>5514.318833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>3037.0</td>\n",
       "      <td>3038.481485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1837</th>\n",
       "      <td>8290.0</td>\n",
       "      <td>8292.794550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual    Predicted\n",
       "1660  3384.0  3382.402704\n",
       "1952  2046.0  2042.917563\n",
       "3075  2744.0  2747.137070\n",
       "1676  1334.0  1335.875165\n",
       "782   4401.0  4400.895905\n",
       "60    3089.0  3083.152282\n",
       "1924  1682.0  1683.184331\n",
       "3074  1680.0  1681.772886\n",
       "1998  2037.0  2035.011556\n",
       "1394   993.0   995.906950\n",
       "104   5520.0  5514.318833\n",
       "769   3037.0  3038.481485\n",
       "1837  8290.0  8292.794550"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparing Actual vs Predicted\n",
    "\n",
    "df2 = pd.DataFrame({'Actual': y_test2, 'Predicted': y_pred2})\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 2.5733533495958736\n",
      "Mean Squared Error: 9.14206617674013\n",
      "Root Mean Squared Error: 3.023584987517323\n"
     ]
    }
   ],
   "source": [
    "# Statistically analyzing model performance\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test2, y_pred2))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test2, y_pred2))\n",
    "print('Root Mean Squared Error:', math.sqrt(metrics.mean_squared_error(y_test2, y_pred2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.3 Data Science\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X and Y values for Finance\n",
    "\n",
    "X3=model_dataset_DS[['Education','Age','Work_experience','Gender']]\n",
    "Y3=model_dataset_DS['Salary']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into test and train\n",
    "        \n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, Y3, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the regression model \n",
    "\n",
    "regressor3 = LinearRegression()\n",
    "regressor3.fit(X_train3, y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making prediction\n",
    "\n",
    "coeff_df3 = pd.DataFrame(regressor3.coef_, X3.columns, columns=['Coefficient'])\n",
    "y_pred3 = regressor3.predict(X_test3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1569</th>\n",
       "      <td>1826.0</td>\n",
       "      <td>1829.639407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2085</th>\n",
       "      <td>7018.0</td>\n",
       "      <td>7021.113522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>4335.0</td>\n",
       "      <td>4340.139357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>1859.0</td>\n",
       "      <td>1855.031028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>4260.0</td>\n",
       "      <td>4253.807848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>1449.0</td>\n",
       "      <td>1444.748936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2054</th>\n",
       "      <td>3162.0</td>\n",
       "      <td>3160.076323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>3174.0</td>\n",
       "      <td>3170.232972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual    Predicted\n",
       "1569  1826.0  1829.639407\n",
       "2085  7018.0  7021.113522\n",
       "1353  4335.0  4340.139357\n",
       "1330  1859.0  1855.031028\n",
       "789   4260.0  4253.807848\n",
       "1483  1449.0  1444.748936\n",
       "2054  3162.0  3160.076323\n",
       "784   3174.0  3170.232972"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparing Actual vs Predicted\n",
    "\n",
    "df3 = pd.DataFrame({'Actual': y_test3, 'Predicted': y_pred3})\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 3.9993974076928964\n",
      "Mean Squared Error: 17.426295015529785\n",
      "Root Mean Squared Error: 4.174481406777348\n"
     ]
    }
   ],
   "source": [
    "# Statistically analyzing model performance\n",
    "\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test3, y_pred3))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test3, y_pred3))\n",
    "print('Root Mean Squared Error:', math.sqrt(metrics.mean_squared_error(y_test3, y_pred3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Making predictions to fix the remaining errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.1 Fixing the Missing Gender for Data Science\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Missing Gender for each sector \n",
    "\n",
    "missing_gender=dirty_data[dirty_data['Gender'].isnull()]\n",
    "missing_gender_IT=missing_gender[missing_gender['Sector']=='IT']\n",
    "missing_gender_Finance=missing_gender[missing_gender['Sector']=='Finance']\n",
    "missing_gender_DS=missing_gender[missing_gender['Sector']=='Data Science']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixing the Missing Gender for Data Science\n",
    "\n",
    "X_G=model_dataset_DS[['Education','Age','Work_experience','Salary']]\n",
    "Y_G=model_dataset_DS['Gender']\n",
    "regressor_g = LinearRegression()\n",
    "regressor_g.fit(X_G, Y_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "coeff_df_g = pd.DataFrame(regressor_g.coef_, X_G.columns, columns=['Coefficient'])\n",
    "X_test_g=missing_gender_DS[['Education','Age','Work_experience','Salary']]\n",
    "y_pred_g=missing_gender_DS['Gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting values \n",
    "y_pred_g = regressor_g.predict(X_test_g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting float predictions to desired format\n",
    "y_pred_g=(list(y_pred_g))\n",
    "for i in range(len(y_pred_g)):\n",
    "    if y_pred_g[i] > 0.5:\n",
    "        y_pred_g[i]=1.0\n",
    "    else:\n",
    "        y_pred_g[i]=0.0\n",
    "        \n",
    "y_pred_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-39-38c748c50f1d>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_gender_DS['Gender']=y_pred_g\n"
     ]
    }
   ],
   "source": [
    "# Applying the predicted values to missing data\n",
    "missing_gender_DS['Gender']=y_pred_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vidhu\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# Applying changes to dirty data \n",
    "\n",
    "for i in range(len(dirty_data)):\n",
    "    for j in range(len(missing_gender_DS)):\n",
    "        if dirty_data['Unnamed: 0'].iloc[i]==missing_gender_DS['Unnamed: 0'].iloc[j]:\n",
    "            dirty_data['Gender'].iloc[i]=missing_gender_DS['Gender'].iloc[j]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.2 Fixing the Missing Gender for IT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixing the Missing Gender for IT\n",
    "\n",
    "X_G_IT=model_dataset_IT[['Education','Age','Work_experience','Salary']]\n",
    "Y_G_IT=model_dataset_IT['Gender']\n",
    "regressor_g_it = LinearRegression()\n",
    "regressor_g_it.fit(X_G, Y_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the above method for IT sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df_g_it = pd.DataFrame(regressor_g_it.coef_, X_G_IT.columns, columns=['Coefficient'])\n",
    "X_test_g_it=missing_gender_IT[['Education','Age','Work_experience','Salary']]\n",
    "y_pred_g_it=missing_gender_IT['Gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "y_pred_g_it = regressor_g_it.predict(X_test_g_it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since our predictions are not exactly in the desired format we make a close approximation \n",
    "y_pred_g_it=(list(y_pred_g_it))\n",
    "for i in range(len(y_pred_g_it)):\n",
    "    if y_pred_g_it[i] > 0.5:\n",
    "        y_pred_g_it[i]=1.0\n",
    "    else:\n",
    "        y_pred_g_it[i]=0.0\n",
    "        \n",
    "y_pred_g_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-142feb09ed66>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_gender_IT['Gender']=y_pred_g_it\n"
     ]
    }
   ],
   "source": [
    "missing_gender_IT['Gender']=y_pred_g_it\n",
    "# missing_gender_IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vidhu\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# Fixing values in dirty data \n",
    "for i in range(len(dirty_data)):\n",
    "    for j in range(len(missing_gender_IT)):\n",
    "        if dirty_data['Unnamed: 0'].iloc[i]==missing_gender_IT['Unnamed: 0'].iloc[j]:\n",
    "            dirty_data['Gender'].iloc[i]=missing_gender_IT['Gender'].iloc[j]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.3 Fixing the Missing Gender for Finance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixing the Missing Gender for Finance\n",
    "\n",
    "X_G_finance=model_dataset_finance[['Education','Age','Work_experience','Salary']]\n",
    "Y_G_finance=model_dataset_finance['Gender']\n",
    "regressor_g_f = LinearRegression()\n",
    "regressor_g_f.fit(X_G_finance, Y_G_finance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df_g_f = pd.DataFrame(regressor_g_f.coef_, X_G_finance.columns, columns=['Coefficient'])\n",
    "X_test_g_f=missing_gender_Finance[['Education','Age','Work_experience','Salary']]\n",
    "y_pred_g_f=missing_gender_Finance['Gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions \n",
    "\n",
    "y_pred_g_f = regressor_g_f.predict(X_test_g_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since our predictions are not exactly in the desired format we make a close approximation \n",
    "\n",
    "y_pred_g_f=(list(y_pred_g_f))\n",
    "for i in range(len(y_pred_g_f)):\n",
    "    if y_pred_g_f[i] > 0.5:\n",
    "        y_pred_g_f[i]=1.0\n",
    "    else:\n",
    "        y_pred_g_f[i]=0.0\n",
    "        \n",
    "y_pred_g_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-3ee25196a697>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_gender_Finance['Gender']=y_pred_g_f\n"
     ]
    }
   ],
   "source": [
    "missing_gender_Finance['Gender']=y_pred_g_f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vidhu\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# Fixing values in dirty data \n",
    "\n",
    "for i in range(len(dirty_data)):\n",
    "    for j in range(len(missing_gender_Finance)):\n",
    "        if dirty_data['Unnamed: 0'].iloc[i]==missing_gender_Finance['Unnamed: 0'].iloc[j]:\n",
    "            dirty_data['Gender'].iloc[i]=missing_gender_Finance['Gender'].iloc[j]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.4 Fixing the Missing Salary for Data Science\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the Missing Gender for Data Science\n",
    "\n",
    "missing_salary=dirty_data[dirty_data['Salary'].isnull()]\n",
    "missing_salary_IT=missing_salary[missing_salary['Sector']=='IT']\n",
    "missing_salary_finace=missing_salary[missing_salary['Sector']=='Finance']\n",
    "missing_salary_DS=missing_salary[missing_salary['Sector']=='Data Science']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the linear regression model \n",
    "\n",
    "X_S_DS=model_dataset_DS[['Education','Age','Work_experience','Gender']]\n",
    "Y_S_DS=model_dataset_DS['Salary']\n",
    "regressor_s_DS = LinearRegression()\n",
    "regressor_s_DS.fit(X_S_DS, Y_S_DS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df_s_DS = pd.DataFrame(regressor_s_DS.coef_, X_S_DS.columns, columns=['Coefficient'])\n",
    "X_test_s_ds=missing_salary_DS[['Education','Age','Work_experience','Gender']]\n",
    "y_pred_s_ds=missing_salary_DS['Salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions \n",
    "y_pred_s_ds = regressor_s_DS.predict(X_test_s_ds)\n",
    "y_pred_s_ds=list(y_pred_s_ds)\n",
    "y_pred_s_ds=[int(x) for x in y_pred_s_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-58-2be8e0ed456a>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_salary_DS['Salary']=y_pred_s_ds\n"
     ]
    }
   ],
   "source": [
    "# Applying the predictions \n",
    "missing_salary_DS['Salary']=y_pred_s_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vidhu\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# Fixing dirty data \n",
    "for i in range(len(dirty_data)):\n",
    "    for j in range(len(missing_salary_DS)):\n",
    "        if dirty_data['Unnamed: 0'].iloc[i]==missing_salary_DS['Unnamed: 0'].iloc[j]:\n",
    "            dirty_data['Salary'].iloc[i]=missing_salary_DS['Salary'].iloc[j]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.5 Fixing the Missing Salary for IT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixing the Missing Salary for IT\n",
    "\n",
    "X_S_IT=model_dataset_IT[['Education','Age','Work_experience','Gender']]\n",
    "Y_S_IT=model_dataset_IT['Salary']\n",
    "\n",
    "# Creating the linear regression model \n",
    "regressor_s_IT = LinearRegression()\n",
    "regressor_s_IT.fit(X_S_IT, Y_S_IT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df_s_IT = pd.DataFrame(regressor_s_IT.coef_, X_S_IT.columns, columns=['Coefficient'])\n",
    "X_test_s_it=missing_salary_IT[['Education','Age','Work_experience','Gender']]\n",
    "y_pred_s_it=missing_salary_IT['Salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions \n",
    "y_pred_s_it = regressor_s_IT.predict(X_test_s_it)\n",
    "y_pred_s_it=list(y_pred_s_it)\n",
    "y_pred_s_it=[int(x) for x in y_pred_s_it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-63-d61f77b5c6ce>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_salary_IT['Salary']=y_pred_s_it\n"
     ]
    }
   ],
   "source": [
    "# Applying those predictions \n",
    "\n",
    "missing_salary_IT['Salary']=y_pred_s_it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vidhu\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# Fixing the dirty data \n",
    "for i in range(len(dirty_data)):\n",
    "    for j in range(len(missing_salary_IT)):\n",
    "        if dirty_data['Unnamed: 0'].iloc[i]==missing_salary_IT['Unnamed: 0'].iloc[j]:\n",
    "            dirty_data['Salary'].iloc[i]=missing_salary_IT['Salary'].iloc[j]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.6 Fixing the Missing Salary for Finace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating model for salary in finance\n",
    "X_S_F=model_dataset_finance[['Education','Age','Work_experience','Gender']]\n",
    "Y_S_F=model_dataset_finance['Salary']\n",
    "regressor_s_F = LinearRegression()\n",
    "regressor_s_F.fit(X_S_F, Y_S_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df_s_F = pd.DataFrame(regressor_s_F.coef_, X_S_F.columns, columns=['Coefficient'])\n",
    "X_test_s_f=missing_salary_finace[['Education','Age','Work_experience','Gender']]\n",
    "y_pred_s_f=missing_salary_finace['Salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions \n",
    "y_pred_s_f = regressor_s_F.predict(X_test_s_f)\n",
    "y_pred_s_f=list(y_pred_s_f)\n",
    "y_pred_s_f=[int(x) for x in y_pred_s_f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-68-2bf79a1d30fd>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_salary_finace['Salary']=y_pred_s_f\n"
     ]
    }
   ],
   "source": [
    "# Applying predictions \n",
    "missing_salary_finace['Salary']=y_pred_s_f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vidhu\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# Fixing values in dirty data \n",
    "for i in range(len(dirty_data)):\n",
    "    for j in range(len(missing_salary_finace)):\n",
    "        if dirty_data['Unnamed: 0'].iloc[i]==missing_salary_finace['Unnamed: 0'].iloc[j]:\n",
    "            dirty_data['Salary'].iloc[i]=missing_salary_finace['Salary'].iloc[j]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.7 Predicting the missing Sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_sector=dirty_data[dirty_data['Sector'].isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting values \n",
    "X_sector=missing_sector[['Education','Age','Work_experience','Gender']]\n",
    "y_sector=missing_sector['Salary']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sepeate model for each sector \n",
    "y_1 = regressor1.predict(X_sector)\n",
    "y_2 = regressor2.predict(X_sector)\n",
    "y_3 = regressor3.predict(X_sector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach -\n",
    "# Since we have the salary of the individuals , we predict the salaries with each model, i.e model created for each sector\n",
    "# The model which predicts the value closest to the actual value is the correct model and thus we get the sector \n",
    "dfx1 = pd.DataFrame({'Actual': y_sector, 'Predicted_IT': y_1,'Predicted_F': y_2,'Predicted_DS': y_3})\n",
    "sector=[]\n",
    "for i in range(len(dfx1)):\n",
    "    # e1 , e2 and e3 mesure the error found between actual value and prediction \n",
    "    e1=dfx1['Actual'].iloc[i]-dfx1['Predicted_IT'].iloc[i]\n",
    "    e2=dfx1['Actual'].iloc[i]-dfx1['Predicted_F'].iloc[i]\n",
    "    e3=dfx1['Actual'].iloc[i]-dfx1['Predicted_DS'].iloc[i]\n",
    "    # Selecting sector that gives minimum erros \n",
    "    if e1==min(e1,e2,e3):\n",
    "        sector.append('IT')\n",
    "    elif e2==min(e1,e2,e3):\n",
    "        sector.append('Finance')\n",
    "    elif e3==min(e1,e2,e3):\n",
    "        sector.append('Data Science')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-74-35e4f1c68d98>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_sector['Sector']=sector\n"
     ]
    }
   ],
   "source": [
    "# Applying predictions \n",
    "missing_sector['Sector']=sector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vidhu\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# Fixing values in dirty data \n",
    "for i in range(len(dirty_data)):\n",
    "    for j in range(len(missing_sector)):\n",
    "        if dirty_data['Unnamed: 0'].iloc[i]==missing_sector['Unnamed: 0'].iloc[j]:\n",
    "            dirty_data['Sector'].iloc[i]=missing_sector['Sector'].iloc[j]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking again if all the missing values are fixed \n",
    "missing_columns=dirty_data.columns[dirty_data.isnull().any()].tolist()\n",
    "missing_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Save the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the resultant dataframe to .csv format\n",
    "dirty_data.to_csv(\"30488141_dirty_data_solution.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<br>\n",
    "    \n",
    "## Task 2 - PARSING AND PRE PROCESSING OF TEXTUAL DATA <a class=\"anchor\" name=\"task1\">\n",
    "    \n",
    "<br>\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Reading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data=open('tweets.txt',encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the file \n",
    "all_tweets_list=[]\n",
    "with open('tweets.txt',  encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        all_tweets_list.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extracting tweets and ids from the data \n",
    "id=[]\n",
    "text=[]\n",
    "\n",
    "for data in all_tweets_list:\n",
    "    for line in re.findall(r'(TWEET::(.*))',data):\n",
    "        text.append(line)\n",
    "        \n",
    "for data in all_tweets_list:\n",
    "    for line in re.findall(r'(ID::(.*))',data):\n",
    "         id.append(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61664"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Storing extracted IDS\n",
    "\n",
    "id_for_Data2=[]\n",
    "\n",
    "for i in range(len(id)):\n",
    "    id_for_Data2.append(id[i][1])\n",
    "\n",
    "len(id_for_Data2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61664"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Storing extracted Texts\n",
    "\n",
    "text_for_Data2=[]\n",
    "\n",
    "for i in range(len(id)):\n",
    "    text_for_Data2.append(text[i][1])\n",
    "\n",
    "len(text_for_Data2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create data set with ID and Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID3374942</td>\n",
       "      <td>@novadairy Ans.5  TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID5514142</td>\n",
       "      <td>Virology lab finds drug originally meant for E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID3503995</td>\n",
       "      <td>#SAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID3487563</td>\n",
       "      <td>Still given time expired, haven't received any...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID7396385</td>\n",
       "      <td>EU Commission President @VonDerLeyen believes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61659</th>\n",
       "      <td>ID5460828</td>\n",
       "      <td>We should keep in mind as we go through battli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61660</th>\n",
       "      <td>ID7436194</td>\n",
       "      <td>Activation of the SARS coronavirus 2revealed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61661</th>\n",
       "      <td>ID7811181</td>\n",
       "      <td>@robertas_world MAD DOG 20-20 WITH A SHOT OF C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61662</th>\n",
       "      <td>ID3281281</td>\n",
       "      <td>Robert Kraft using Patriots plane to airlift c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61663</th>\n",
       "      <td>ID7274390</td>\n",
       "      <td>Cuomo should be one of the most loathed offici...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61664 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                               text\n",
       "0      ID3374942                           @novadairy Ans.5  TRUE\n",
       "1      ID5514142  Virology lab finds drug originally meant for E...\n",
       "2      ID3503995                                              #SAR \n",
       "3      ID3487563  Still given time expired, haven't received any...\n",
       "4      ID7396385  EU Commission President @VonDerLeyen believes ...\n",
       "...          ...                                                ...\n",
       "61659  ID5460828  We should keep in mind as we go through battli...\n",
       "61660  ID7436194  Activation of the SARS coronavirus 2revealed ...\n",
       "61661  ID7811181  @robertas_world MAD DOG 20-20 WITH A SHOT OF C...\n",
       "61662  ID3281281  Robert Kraft using Patriots plane to airlift c...\n",
       "61663  ID7274390  Cuomo should be one of the most loathed offici...\n",
       "\n",
       "[61664 rows x 2 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets_ordered2 =pd.DataFrame({'id': id_for_Data2,'text': text_for_Data2})\n",
    "all_tweets_ordered2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Add Sentiment to each tweet in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the number of positive tweets for each user \n",
    "boolean=[]\n",
    "tweet2=[]\n",
    "sid=SIA()\n",
    "k=list(set(list(all_tweets_ordered2['id'])))\n",
    "for i in range(len(k)):\n",
    "    s=[]\n",
    "    positive_tweet=0\n",
    "    for x in range(len(all_tweets_ordered2['id'])):\n",
    "        if all_tweets_ordered2['id'][x]==k[i]:\n",
    "            s.append(str(all_tweets_ordered2['text'][x])+\",\")\n",
    "            #Calculating polarity score\n",
    "            ss=sid.polarity_scores(all_tweets_ordered2['text'][x])\n",
    "            # If score is greater than 0.05 its positive \n",
    "            if ss['compound']>0.05:\n",
    "                positive_tweet=positive_tweet+1\n",
    "            else:\n",
    "                positive_tweet=positive_tweet+0\n",
    "    \n",
    "    boolean.append(positive_tweet)\n",
    "    tweet2.append(s)\n",
    "    \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for further analysis \n",
    "data2 =pd.DataFrame({'ID': k,'text': tweet2,'Positive_tweets_no': boolean})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for tweets_json \n",
    "data =pd.DataFrame({'id': k,'text': tweet2})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Save the Tweet file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_json ('tweets.json',orient='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the two datasets \n",
    "new_Dataset=pd.merge(dirty_data, data2, on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-89-39f417cc83eb>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_Dataset['Lan'][i]=lan[i][0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lan=[]\n",
    "new_Dataset['Lan']=''\n",
    "for i in range(len(new_Dataset['text'])):\n",
    "    lan.append(langid.classify(str(new_Dataset['text'][i])))\n",
    "    new_Dataset['Lan'][i]=lan[i][0]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Create unigram tokens \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the list of stop words\n",
    "stopwords=[]\n",
    "stopwords_list=open(\"stopwords_en.txt\",\"r\")\n",
    "for x in stopwords_list.readlines():\n",
    "    stopwords.append(x)\n",
    "    stopwords=list(map(lambda x:x.strip(),stopwords))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sepearate datasets by sectors\n",
    "new_Dataset_IT=new_Dataset[new_Dataset['Sector']=='IT']\n",
    "new_Dataset_F=new_Dataset[new_Dataset['Sector']=='Finance']\n",
    "new_Dataset_DS=new_Dataset[new_Dataset['Sector']=='Data Science']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "new_Dataset_IT=new_Dataset_IT.reset_index()\n",
    "new_Dataset_DS=new_Dataset_DS.reset_index()\n",
    "new_Dataset_F=new_Dataset_F.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying porter stemmer to trim down the tweets\n",
    "porter_stemmer=PorterStemmer()\n",
    "trimmed_tweets_DS=[]\n",
    "for i in range(len(new_Dataset_DS['text'])):\n",
    "    tweets = ''.join(new_Dataset_DS['text'][i])\n",
    "    tokens=tweets.split()\n",
    "    stemmed_tokens=[porter_stemmer.stem(token) for token in tokens]\n",
    "    tweet=' '.join(stemmed_tokens)\n",
    "    trimmed_tweets_DS.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying porter stemmer to trim down the tweets\n",
    "\n",
    "porter_stemmer=PorterStemmer()\n",
    "trimmed_tweets_IT=[]\n",
    "for i in range(len(new_Dataset_IT['text'])):\n",
    "    tweets = ''.join(new_Dataset_IT['text'][i])\n",
    "    tokens=tweets.split()\n",
    "    stemmed_tokens=[porter_stemmer.stem(token) for token in tokens]\n",
    "    tweet=' '.join(stemmed_tokens)\n",
    "    trimmed_tweets_IT.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying porter stemmer to trim down the tweets\n",
    "\n",
    "porter_stemmer=PorterStemmer()\n",
    "trimmed_tweets_F=[]\n",
    "for i in range(len(new_Dataset_F['text'])):\n",
    "    tweets = ''.join(new_Dataset_F['text'][i])\n",
    "    tokens=tweets.split()\n",
    "    stemmed_tokens=[porter_stemmer.stem(token) for token in tokens]\n",
    "    tweet=' '.join(stemmed_tokens)\n",
    "    trimmed_tweets_F.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hunt',\n",
       " 'ventil',\n",
       " 'medic',\n",
       " 'suppli',\n",
       " 'consum',\n",
       " 'europ',\n",
       " 'america',\n",
       " 'coronaviru',\n",
       " 'infect',\n",
       " 'soar',\n",
       " 'polit',\n",
       " 'paralysi',\n",
       " 'stall',\n",
       " 'effort',\n",
       " 'quick',\n",
       " 'packag',\n",
       " 'congress',\n",
       " 'https',\n",
       " 'gngn',\n",
       " 'covid',\n",
       " 'updat',\n",
       " 'april',\n",
       " 'check',\n",
       " 'fiverr',\n",
       " 'bring',\n",
       " 'traffic',\n",
       " 'websit',\n",
       " 'market',\n",
       " 'promot',\n",
       " 'https',\n",
       " 'jgkucv',\n",
       " 'coronaviru',\n",
       " 'covid',\n",
       " 'important',\n",
       " 'https',\n",
       " 'zpgxnygbab',\n",
       " 'kenaidel',\n",
       " 'johnfielder',\n",
       " 'tomlondon',\n",
       " 'keir',\n",
       " 'starm',\n",
       " 'boardofdeputi',\n",
       " 'chiefrabbi',\n",
       " 'jewishlabour',\n",
       " 'forc',\n",
       " 'propaganda',\n",
       " 'side',\n",
       " 'anoth',\n",
       " 'point',\n",
       " 'view',\n",
       " 'elect',\n",
       " 'peopl',\n",
       " 'offer',\n",
       " 'genuin',\n",
       " 'choice',\n",
       " 'covid',\n",
       " 'shown',\n",
       " 'prescienc',\n",
       " 'offer',\n",
       " 'nutcase',\n",
       " 'guess',\n",
       " 'miss',\n",
       " 'daili',\n",
       " 'accident',\n",
       " 'https',\n",
       " 'zqdsw',\n",
       " 'arrest',\n",
       " 'nigga',\n",
       " 'https',\n",
       " 'poiof',\n",
       " 'vfln',\n",
       " 'https',\n",
       " 'rejkpzokkk',\n",
       " 'https',\n",
       " 'qppochy',\n",
       " 'slow',\n",
       " 'spread',\n",
       " 'covid',\n",
       " 'identifi',\n",
       " 'risk',\n",
       " 'case',\n",
       " 'sooner',\n",
       " 'self-report',\n",
       " 'symptom',\n",
       " 'daily',\n",
       " 'feel',\n",
       " 'download',\n",
       " 'https',\n",
       " 'lnvr',\n",
       " 'mdnij',\n",
       " 'trump',\n",
       " 'probabl',\n",
       " 'sleep',\n",
       " 'veri',\n",
       " 'becaus',\n",
       " 'care',\n",
       " 'sick',\n",
       " 'death',\n",
       " 'coronaviru',\n",
       " 'becaus',\n",
       " 'inaction',\n",
       " 'feel',\n",
       " 'hear',\n",
       " 'patient',\n",
       " 'round',\n",
       " 'realli',\n",
       " 'appreciated',\n",
       " 'doctor',\n",
       " 'caught',\n",
       " 'coronaviru',\n",
       " 'explain',\n",
       " 'symptom',\n",
       " 'day-by',\n",
       " 'show',\n",
       " 'ultrasound',\n",
       " 'scan',\n",
       " 'lung',\n",
       " 'https',\n",
       " 'oabd',\n",
       " 'someon',\n",
       " 'supposedli',\n",
       " 'swore',\n",
       " 'protect',\n",
       " 'give',\n",
       " 'https',\n",
       " 'fnokgjnkiz',\n",
       " 'report',\n",
       " 'case',\n",
       " 'coronaviru',\n",
       " 'deaths',\n",
       " 'sunlorri',\n",
       " 'hurry',\n",
       " 'coronaviru',\n",
       " 'rear',\n",
       " 'ugli',\n",
       " 'head',\n",
       " 'onion',\n",
       " 'https',\n",
       " 'cwctslvkhh',\n",
       " 'stud',\n",
       " 'confid',\n",
       " 'school',\n",
       " 'assur',\n",
       " 'safety',\n",
       " 'covid',\n",
       " 'threat',\n",
       " 'guess',\n",
       " 'skip',\n",
       " 'school',\n",
       " 'year',\n",
       " 'emaar',\n",
       " 'india',\n",
       " 'appoint',\n",
       " 'peregrinesecur',\n",
       " 'fali',\n",
       " 'delay',\n",
       " 'monthli',\n",
       " 'wage',\n",
       " 'guard',\n",
       " 'gurgaon',\n",
       " 'round',\n",
       " 'discuss',\n",
       " 'released',\n",
       " 'intervent',\n",
       " 'needed',\n",
       " 'thread',\n",
       " 'https',\n",
       " 'nqxtuegkqk',\n",
       " 'care',\n",
       " 'stay',\n",
       " 'orangefuckfac',\n",
       " 'good',\n",
       " 'side',\n",
       " 'realli',\n",
       " 'wishes',\n",
       " 'https',\n",
       " 'skcfd',\n",
       " 'coronaviru',\n",
       " 'pandam',\n",
       " 'give',\n",
       " 'time',\n",
       " 'reflect',\n",
       " 'worst',\n",
       " 'enemy',\n",
       " 'lockdown',\n",
       " 'start',\n",
       " 'test',\n",
       " 'mizoram',\n",
       " 'lockdown',\n",
       " 'covid',\n",
       " 'priv',\n",
       " 'hospit',\n",
       " 'warn',\n",
       " 'overcharg',\n",
       " 'patient',\n",
       " 'covid',\n",
       " 'capit',\n",
       " 'https',\n",
       " 'fmfxoe',\n",
       " 'role',\n",
       " 'play',\n",
       " 'virus',\n",
       " 'track',\n",
       " 'covid',\n",
       " 'india',\n",
       " 'https',\n",
       " 'consum',\n",
       " 'group',\n",
       " 'call',\n",
       " 'lower',\n",
       " 'insur',\n",
       " 'premium',\n",
       " 'road',\n",
       " 'fall',\n",
       " 'silent',\n",
       " 'daili',\n",
       " 'https',\n",
       " 'qbzwf',\n",
       " 'quick',\n",
       " 'maga',\n",
       " 'secpompeo',\n",
       " 'pompeo',\n",
       " 'enorm',\n",
       " 'evidence',\n",
       " 'coronaviru',\n",
       " 'chines',\n",
       " 'trump',\n",
       " 'coronaviru',\n",
       " 'reelect',\n",
       " 'effort',\n",
       " 'https',\n",
       " 'clgb',\n",
       " 'msnbc',\n",
       " 'cancel',\n",
       " 'arttrad',\n",
       " 'person',\n",
       " 'matter',\n",
       " 'covid',\n",
       " 'quarantine',\n",
       " 'poetri',\n",
       " 'coronaviru',\n",
       " 'https',\n",
       " 'pvtwffznkt',\n",
       " 'wedemandstipendincr',\n",
       " 'https',\n",
       " 'gvkmwc',\n",
       " 'govern',\n",
       " 'encourag',\n",
       " 'privat',\n",
       " 'invest',\n",
       " 'renew',\n",
       " 'give',\n",
       " 'green',\n",
       " 'light',\n",
       " 'project',\n",
       " 'address',\n",
       " 'climatechange',\n",
       " 'read',\n",
       " 'letter',\n",
       " 'borisjohnson',\n",
       " 'call',\n",
       " 'green',\n",
       " 'covid',\n",
       " 'econom',\n",
       " 'recovery',\n",
       " 'https',\n",
       " 'gcvj',\n",
       " 'realli',\n",
       " 'good',\n",
       " 'podcast',\n",
       " 'ugle',\n",
       " 'drdstapl',\n",
       " 'explain',\n",
       " 'freemason',\n",
       " 'https',\n",
       " 'tuiuk',\n",
       " 'martinslewi',\n",
       " 'whichuk',\n",
       " 'tuiuk',\n",
       " 'confirm',\n",
       " 'holiday',\n",
       " 'departur',\n",
       " 'date',\n",
       " 'befor',\n",
       " 'april',\n",
       " 'refund',\n",
       " 'https',\n",
       " 'mnao',\n",
       " 'rylet',\n",
       " 'nojusticenopeaceprosecutethepolic',\n",
       " 'blacklivesmatt',\n",
       " 'anonym',\n",
       " 'https',\n",
       " 'zagvoa',\n",
       " 'spiegel',\n",
       " 'politik',\n",
       " 'kuku',\n",
       " 'angela',\n",
       " 'merkel',\n",
       " 'peopl',\n",
       " 'germani',\n",
       " 'patient',\n",
       " 'relax',\n",
       " 'measur',\n",
       " 'coronaviru',\n",
       " 'early',\n",
       " 'chancellor',\n",
       " 'veri',\n",
       " 'true',\n",
       " 'mess',\n",
       " 'surpris',\n",
       " 'number',\n",
       " 'rise',\n",
       " 'covid',\n",
       " 'case',\n",
       " 'evri',\n",
       " 'major',\n",
       " 'direct',\n",
       " 'ncdc',\n",
       " 'strictli',\n",
       " 'adher',\n",
       " 'https',\n",
       " 'dibmap',\n",
       " 'realli',\n",
       " 'hell',\n",
       " 'yougov',\n",
       " 'reach',\n",
       " 'https',\n",
       " 'xbpeecmzyv',\n",
       " 'coronaviru',\n",
       " 'worldwid',\n",
       " 'case',\n",
       " 'cross',\n",
       " 'million',\n",
       " 'mark',\n",
       " 'death',\n",
       " 'toll',\n",
       " 'https',\n",
       " 'othbru',\n",
       " 'enrol',\n",
       " 'coronaviru',\n",
       " 'contact',\n",
       " 'trace',\n",
       " 'academi',\n",
       " 'https',\n",
       " 'kiptchkwna',\n",
       " 'https',\n",
       " 'yunjv',\n",
       " 'chbd',\n",
       " 'planetamerica',\n",
       " 'bear',\n",
       " 'moment',\n",
       " 'democrat',\n",
       " 'honestli',\n",
       " 'heard',\n",
       " 'stori',\n",
       " 'scorpion',\n",
       " 'frog',\n",
       " 'https',\n",
       " 'hgfyqpoj',\n",
       " 'kirosmix',\n",
       " 'june',\n",
       " 'preview',\n",
       " 'kinda',\n",
       " 'obviou',\n",
       " 'voluntari',\n",
       " 'social',\n",
       " 'distanc',\n",
       " 'hygien',\n",
       " 'fairli',\n",
       " 'effect',\n",
       " 'https',\n",
       " 'bevffjsrnw',\n",
       " 'donaldtrump',\n",
       " 'coronaviru',\n",
       " 'covid',\n",
       " 'covid',\n",
       " 'studi',\n",
       " 'show',\n",
       " 'viru',\n",
       " 'start',\n",
       " 'earlier',\n",
       " 'thought',\n",
       " 'coronaviru',\n",
       " 'https',\n",
       " 'magic',\n",
       " 'https',\n",
       " 'njnenwcdsz',\n",
       " 'ocado',\n",
       " 'issu',\n",
       " 'refund',\n",
       " 'mislabel',\n",
       " 'face',\n",
       " 'mask',\n",
       " 'standard',\n",
       " 'match',\n",
       " 'exact',\n",
       " 'specif',\n",
       " 'stated',\n",
       " 'hindu',\n",
       " 'world',\n",
       " 'face',\n",
       " 'worst',\n",
       " 'crisi',\n",
       " 'sinc',\n",
       " 'great',\n",
       " 'depression',\n",
       " 'chief',\n",
       " 'mourn',\n",
       " 'dead',\n",
       " 'covid',\n",
       " 'pandem',\n",
       " 'mani',\n",
       " 'cultur',\n",
       " 'tradit',\n",
       " 'griev',\n",
       " 'possibl',\n",
       " 'times',\n",
       " 'impact',\n",
       " 'griev',\n",
       " 'individu',\n",
       " 'famili',\n",
       " 'communities',\n",
       " 'bereav',\n",
       " 'support',\n",
       " 'https',\n",
       " 'nnvp',\n",
       " 'owira',\n",
       " 'peopl',\n",
       " 'yahoonew',\n",
       " 'https',\n",
       " 'vrfk',\n",
       " 'zsmmr',\n",
       " 'expert',\n",
       " 'worried',\n",
       " 'mentalhealth',\n",
       " 'give',\n",
       " 'stay',\n",
       " 'sane',\n",
       " 'lockdown',\n",
       " 'durham',\n",
       " 'https',\n",
       " 'isqbv',\n",
       " 'pird',\n",
       " 'trump',\n",
       " 'halt',\n",
       " 'world',\n",
       " 'health',\n",
       " 'organ',\n",
       " 'fund',\n",
       " 'coronaviru',\n",
       " 'failure',\n",
       " 'singapor',\n",
       " 'approv',\n",
       " 'remdesivir',\n",
       " 'drug',\n",
       " 'emerg',\n",
       " 'covid',\n",
       " 'treatment',\n",
       " 'answer',\n",
       " 'live',\n",
       " 'coffin',\n",
       " 'https',\n",
       " 'qlcod',\n",
       " 'policechiefs',\n",
       " 'report',\n",
       " 'crime',\n",
       " 'dure',\n",
       " 'coronaviru',\n",
       " 'outbreak',\n",
       " 'lizzard',\n",
       " 'micaylahodg',\n",
       " 'stealthygeek',\n",
       " 'heck',\n",
       " 'doctor',\n",
       " 'light',\n",
       " 'kill',\n",
       " 'coronaviru',\n",
       " 'human',\n",
       " 'light',\n",
       " 'cell',\n",
       " 'body',\n",
       " 'disinfect',\n",
       " 'lungs',\n",
       " 'love',\n",
       " 'give',\n",
       " 'shout',\n",
       " 'lothianbus',\n",
       " 'dure',\n",
       " 'covid',\n",
       " 'epidemic',\n",
       " 'replac',\n",
       " 'green',\n",
       " 'buse',\n",
       " 'countri',\n",
       " 'coastal',\n",
       " 'rout',\n",
       " 'buses',\n",
       " 'becaus',\n",
       " 'buse',\n",
       " 'screen',\n",
       " 'protect',\n",
       " 'drivers',\n",
       " 'phocuswire',\n",
       " 'live',\n",
       " 'blog',\n",
       " 'peopl',\n",
       " 'ride-shar',\n",
       " 'servic',\n",
       " 'post-coronavirus',\n",
       " 'visit',\n",
       " 'royal',\n",
       " 'brompton',\n",
       " 'harefield',\n",
       " 'lung',\n",
       " 'transplant',\n",
       " 'clinic',\n",
       " 'clinic',\n",
       " 'current',\n",
       " 'cancel',\n",
       " 'becaus',\n",
       " 'danger',\n",
       " 'covid',\n",
       " 'pose',\n",
       " 'patient',\n",
       " 'wait',\n",
       " 'list',\n",
       " 'mani',\n",
       " 'cystic',\n",
       " 'fibrosi',\n",
       " 'https',\n",
       " 'rhqd',\n",
       " 'nasa',\n",
       " 'starbucks',\n",
       " 'vice',\n",
       " 'president',\n",
       " 'piersmorgan',\n",
       " 'https',\n",
       " 'epbz',\n",
       " 'model',\n",
       " 'crappy',\n",
       " 'hoax',\n",
       " 'https',\n",
       " 'uyfbv',\n",
       " 'ehbp',\n",
       " 'cuck',\n",
       " 'todd',\n",
       " 'https',\n",
       " 'cfhratgzzl',\n",
       " 'relax',\n",
       " 'lockdown',\n",
       " 'upend',\n",
       " 'everyday',\n",
       " 'life',\n",
       " 'https',\n",
       " 'uncmtqim',\n",
       " 'milli',\n",
       " 'vanilliii',\n",
       " 'sirhottest',\n",
       " 'welcom',\n",
       " 'future',\n",
       " 'tech',\n",
       " 'thought',\n",
       " 'pictur',\n",
       " 'noth',\n",
       " 'coronavirus',\n",
       " 'murder',\n",
       " 'hornets',\n",
       " 'murder',\n",
       " 'hornet',\n",
       " 'carri',\n",
       " 'coronavirus',\n",
       " 'work',\n",
       " 'irrat',\n",
       " 'frenzi',\n",
       " 'pleas',\n",
       " 'revert',\n",
       " 'back',\n",
       " 'regular',\n",
       " 'sensation',\n",
       " 'click',\n",
       " 'bait',\n",
       " 'https',\n",
       " 'udyhyh',\n",
       " 'coronaviru',\n",
       " 'join',\n",
       " 'lawsuit',\n",
       " 'china',\n",
       " 'https',\n",
       " 'jhufrd',\n",
       " 'brogodzilla',\n",
       " 'logic',\n",
       " 'creat',\n",
       " 'covid',\n",
       " 'shripadynaik',\n",
       " 'video',\n",
       " 'confer',\n",
       " 'facil',\n",
       " 'provid',\n",
       " 'chennai',\n",
       " 'siddha',\n",
       " 'praction',\n",
       " 'india',\n",
       " 'interact',\n",
       " 'chennai',\n",
       " 'understand',\n",
       " 'experi',\n",
       " 'covid',\n",
       " 'patient',\n",
       " 'treatment',\n",
       " 'posit',\n",
       " 'results',\n",
       " 'kind',\n",
       " 'blame',\n",
       " 'lame',\n",
       " 'offici',\n",
       " 'word',\n",
       " 'testing',\n",
       " 'test',\n",
       " 'visitor',\n",
       " 'forc',\n",
       " 'quarantin',\n",
       " 'results',\n",
       " 'make',\n",
       " 'test',\n",
       " 'quarantin',\n",
       " 'part',\n",
       " 'trip',\n",
       " 'https',\n",
       " 'fwzjw',\n",
       " 'update',\n",
       " 'dubai',\n",
       " 'evacue',\n",
       " 'test',\n",
       " 'posit',\n",
       " 'covid',\n",
       " 'https',\n",
       " 'jsorc',\n",
       " 'https',\n",
       " 'sjkuci',\n",
       " 'wrong',\n",
       " 'stay',\n",
       " 'arent',\n",
       " 'sick',\n",
       " 'chanc',\n",
       " 'https',\n",
       " 'iebb',\n",
       " 'horr',\n",
       " 'https',\n",
       " 'busi',\n",
       " 'enorm',\n",
       " 'evidence',\n",
       " 'origin',\n",
       " 'coronavirus',\n",
       " 'presid',\n",
       " 'administr',\n",
       " 'defin',\n",
       " 'lies',\n",
       " 'https',\n",
       " 'sunhpkstdx',\n",
       " 'year-old',\n",
       " 'woman',\n",
       " 'surviv',\n",
       " 'coronaviru',\n",
       " 'warn',\n",
       " 'human',\n",
       " 'order',\n",
       " 'https',\n",
       " 'hllyllm',\n",
       " 'https',\n",
       " 'zbfbho',\n",
       " 'exactli',\n",
       " 'democrat',\n",
       " 'deranged',\n",
       " 'predictable',\n",
       " 'https',\n",
       " 'fapf',\n",
       " 'zokez',\n",
       " 'reason',\n",
       " 'high',\n",
       " 'number',\n",
       " 'fatal',\n",
       " 'black',\n",
       " 'minor',\n",
       " 'health',\n",
       " 'staff',\n",
       " 'exclusive',\n",
       " 'nurs',\n",
       " 'feel',\n",
       " 'targeted',\n",
       " 'work',\n",
       " 'covid',\n",
       " 'ward',\n",
       " 'https',\n",
       " 'zshlzfu',\n",
       " 'tablighi',\n",
       " 'jamaat',\n",
       " 'ruptur',\n",
       " 'india',\n",
       " 'coronaviru',\n",
       " 'shield',\n",
       " 'case',\n",
       " 'link',\n",
       " 'tablighi',\n",
       " 'jamaat',\n",
       " 'states',\n",
       " 'bantablighijamat',\n",
       " 'coronaviru',\n",
       " 'update',\n",
       " 'rubi',\n",
       " 'princess',\n",
       " 'passeng',\n",
       " 'nearli',\n",
       " 'tasmanian',\n",
       " 'forc',\n",
       " 'quarantin',\n",
       " 'https',\n",
       " 'mjgjwyzdrn',\n",
       " 'abcnew',\n",
       " 'covid',\n",
       " 'themohkum',\n",
       " 'possibl',\n",
       " \"you'r\",\n",
       " 'asymptomatic',\n",
       " 'isol',\n",
       " 'covid',\n",
       " 'nasal',\n",
       " 'test',\n",
       " 'accur',\n",
       " 'test',\n",
       " 'negative',\n",
       " 'isol',\n",
       " 'victoriapolic',\n",
       " 'onli',\n",
       " 'pick',\n",
       " 'soft',\n",
       " 'targets',\n",
       " 'crowd',\n",
       " 'anti-riot',\n",
       " 'truck',\n",
       " 'danielandrewsmp',\n",
       " 'https',\n",
       " 'kyteiuh',\n",
       " 'releas',\n",
       " 'gekijo',\n",
       " 'star',\n",
       " 'yamazaki',\n",
       " 'kento',\n",
       " 'matsuoka',\n",
       " 'mayu',\n",
       " 'postpon',\n",
       " 'result',\n",
       " 'coronaviru',\n",
       " 'outbreak',\n",
       " 'https',\n",
       " 'cpmlqe',\n",
       " 'folks',\n",
       " 'thttps',\n",
       " 'foxnews',\n",
       " 'media',\n",
       " 'msnbc-host',\n",
       " 'suggests-biden',\n",
       " 'form-shadow',\n",
       " 'government-to',\n",
       " 'counter-trump',\n",
       " 'on-coronavirus',\n",
       " 'hostil',\n",
       " 'hamper',\n",
       " 'respons',\n",
       " 'covid',\n",
       " 'pandemic',\n",
       " 'plan',\n",
       " 'establish',\n",
       " 'isol',\n",
       " 'hold',\n",
       " 'ongo',\n",
       " 'insecurity',\n",
       " 'monitor',\n",
       " 'situat',\n",
       " 'close',\n",
       " 'remain',\n",
       " 'close',\n",
       " 'contact',\n",
       " 'field',\n",
       " 'coordin',\n",
       " 'insid',\n",
       " 'tarhouna',\n",
       " 'covid',\n",
       " 'treatment',\n",
       " 'remdesivir',\n",
       " 'distributed',\n",
       " 'price',\n",
       " 'patent',\n",
       " 'drugs',\n",
       " 'post',\n",
       " 'resach',\n",
       " 'jsherkow',\n",
       " 'wnicholsonpric',\n",
       " 'https',\n",
       " 'zronzuohpk',\n",
       " 'teamtrump',\n",
       " 'realdonaldtrump',\n",
       " 'impeach',\n",
       " 'articl',\n",
       " 'transfer',\n",
       " 'senate',\n",
       " 'anncoult',\n",
       " 'nearli',\n",
       " 'year',\n",
       " 'presid',\n",
       " 'trump',\n",
       " 'attack',\n",
       " 'fake',\n",
       " 'impeach',\n",
       " 'hoax',\n",
       " 'smear',\n",
       " 'chines',\n",
       " 'covid',\n",
       " 'elect',\n",
       " 'year',\n",
       " 'talk',\n",
       " 'compassionate',\n",
       " 'nytim',\n",
       " 'nytim',\n",
       " 'onli',\n",
       " 'pictur',\n",
       " 'illustr',\n",
       " 'covid',\n",
       " 'stories',\n",
       " 'priyankagandhi',\n",
       " 'solicit',\n",
       " 'italians',\n",
       " 'sound',\n",
       " 'expect',\n",
       " 'back',\n",
       " 'normal',\n",
       " 'anytim',\n",
       " 'econom',\n",
       " 'devast',\n",
       " 'situation',\n",
       " 'focu',\n",
       " 'turn',\n",
       " 'protect',\n",
       " 'people',\n",
       " 'livelihood',\n",
       " 'lives',\n",
       " 'mani',\n",
       " 'lose',\n",
       " 'incom',\n",
       " 'fault',\n",
       " 'https',\n",
       " 'uwhkoem',\n",
       " 'https',\n",
       " 'avfsqndvse',\n",
       " 'laundri',\n",
       " 'expos',\n",
       " 'fake',\n",
       " 'articl',\n",
       " 'https',\n",
       " 'liwpeeybl',\n",
       " 'lamest',\n",
       " 'constitut',\n",
       " 'crisi',\n",
       " 'history',\n",
       " 'https',\n",
       " 'zdukeglubq',\n",
       " 'analys',\n",
       " 'latest',\n",
       " 'data',\n",
       " 'continu',\n",
       " 'monitor',\n",
       " 'impact',\n",
       " 'covid',\n",
       " 'aviat',\n",
       " 'industry',\n",
       " 'onsit',\n",
       " 'coronaviru',\n",
       " 'case',\n",
       " 'secret',\n",
       " 'scare',\n",
       " 'worker',\n",
       " 'told',\n",
       " 'construct',\n",
       " 'https',\n",
       " 'qnkbwpy',\n",
       " 'philadelphia',\n",
       " 'film',\n",
       " 'societi',\n",
       " 'deni',\n",
       " 'worker',\n",
       " 'paid',\n",
       " 'sick',\n",
       " 'time',\n",
       " 'coronaviru',\n",
       " 'brought',\n",
       " 'issu',\n",
       " 'head',\n",
       " 'https',\n",
       " 'masdyc',\n",
       " 'revealed',\n",
       " 'leader',\n",
       " 'group',\n",
       " 'peddl',\n",
       " 'bleach',\n",
       " 'coronaviru',\n",
       " 'cure',\n",
       " 'wrote',\n",
       " 'trump',\n",
       " 'week',\n",
       " 'https',\n",
       " 'xkccz',\n",
       " 'maga',\n",
       " 'maga',\n",
       " 'trump',\n",
       " 'keepamericagreat',\n",
       " 'coronaviru',\n",
       " 'coronavirusupd',\n",
       " 'covidiot',\n",
       " 'coronawarrior',\n",
       " 'https',\n",
       " 'jointhefightback',\n",
       " 'bayanihantohealasone',\n",
       " 'nsvirk',\n",
       " 'police',\n",
       " 'haryana',\n",
       " 'gurgaonpolic',\n",
       " 'fbdpolic',\n",
       " 'panchkula',\n",
       " 'anilvijminist',\n",
       " 'mlkhattar',\n",
       " 'cmohri',\n",
       " 'timesofindia',\n",
       " 'great',\n",
       " 'covid',\n",
       " 'joke',\n",
       " 'type',\n",
       " 'peopl',\n",
       " 'follow',\n",
       " 'govt',\n",
       " 'orders',\n",
       " 'polic',\n",
       " 'great',\n",
       " 'llnews',\n",
       " 'offic',\n",
       " 'mike',\n",
       " 'vreek',\n",
       " 'found',\n",
       " 'guilti',\n",
       " 'misconduct',\n",
       " 'scot',\n",
       " 'discov',\n",
       " 'covid',\n",
       " 'antidote',\n",
       " 'buckfast',\n",
       " 'cure',\n",
       " 'maladi',\n",
       " 'sinc',\n",
       " 'https',\n",
       " 'fpfp',\n",
       " 'zzowa',\n",
       " 'class',\n",
       " 'bunch',\n",
       " 'peopl',\n",
       " 'fail',\n",
       " 'test',\n",
       " 'poor',\n",
       " 'grade',\n",
       " 'insist',\n",
       " 'fail',\n",
       " 'test',\n",
       " 'peopl',\n",
       " 'fail',\n",
       " 'quietli',\n",
       " 'seat',\n",
       " 'ahead',\n",
       " 'curve',\n",
       " 'nationb',\n",
       " 'ashwani',\n",
       " 'mahajan',\n",
       " 'potu',\n",
       " 'full',\n",
       " 'excerpt',\n",
       " 'https',\n",
       " 'uvjq',\n",
       " 'inform',\n",
       " 'result',\n",
       " 'clinic',\n",
       " 'test',\n",
       " 'provid',\n",
       " 'confirm',\n",
       " 'observ',\n",
       " 'transmiss',\n",
       " 'famili',\n",
       " 'members',\n",
       " 'headsup',\n",
       " 'pneumonia',\n",
       " 'unknown',\n",
       " 'updateatnoon',\n",
       " 'sakinakamwendo',\n",
       " 'druggi',\n",
       " ...]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Regex to indetify for tokenization \n",
    "\n",
    "tweets_tokenized_IT={}\n",
    "tokenizer_IT=RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "raw_text=\"\" \n",
    "for item2 in trimmed_tweets_IT:\n",
    "    raw_text=raw_text+str(item2)\n",
    "    raw_text=raw_text.lower()\n",
    "    unigram_tokens_IT=tokenizer_IT.tokenize(raw_text)\n",
    "    # Applying condition for length greater than 3 and lower bound \n",
    "    unigram_tokens_IT=[value for index,value in enumerate(unigram_tokens_IT) if len(value)>3 and value.lower() not in stopwords]\n",
    "unigram_tokens_IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['postal',\n",
       " 'inspector',\n",
       " 'caution',\n",
       " 'covid',\n",
       " 'scam',\n",
       " 'request',\n",
       " 'number',\n",
       " 'info',\n",
       " 'including',\n",
       " 'algeria',\n",
       " 'sanitari',\n",
       " 'educ',\n",
       " 'police',\n",
       " 'sanction',\n",
       " 'polic',\n",
       " 'offic',\n",
       " 'explain',\n",
       " 'risk',\n",
       " 'spread',\n",
       " 'coronaviru',\n",
       " 'epidem',\n",
       " 'elderli',\n",
       " 'passerbi',\n",
       " 'bilad',\n",
       " 'nytopinion',\n",
       " 'https',\n",
       " 'fibcy',\n",
       " 'compar',\n",
       " 'month',\n",
       " 'number',\n",
       " 'patient',\n",
       " 'drop',\n",
       " 'number',\n",
       " 'intub',\n",
       " 'patient',\n",
       " 'number',\n",
       " 'daili',\n",
       " 'case',\n",
       " 'reopen',\n",
       " 'fight',\n",
       " 'good',\n",
       " 'fight',\n",
       " 'covid',\n",
       " 'saglikbakanligi',\n",
       " 'drfahrettinkoca',\n",
       " 'https',\n",
       " 'bethanyallenebr',\n",
       " 'stupid',\n",
       " 'assumpt',\n",
       " 'chines',\n",
       " 'uncivil',\n",
       " 'american',\n",
       " 'attack',\n",
       " 'chines',\n",
       " 'becaus',\n",
       " 'covid',\n",
       " 'discov',\n",
       " 'china',\n",
       " 'utang',\n",
       " 'https',\n",
       " 'https',\n",
       " 'srodlskd',\n",
       " 'covid',\n",
       " 'report',\n",
       " 'cross',\n",
       " 'river',\n",
       " 'govt',\n",
       " 'buhari',\n",
       " 'https',\n",
       " 'vmozkmhisi',\n",
       " 'covid',\n",
       " 'hkogi',\n",
       " 'strongertogether',\n",
       " 'adambienkov',\n",
       " 'domin',\n",
       " 'sentimental',\n",
       " 'covid',\n",
       " 'relat',\n",
       " 'death',\n",
       " 'suggest',\n",
       " 'coronavirus',\n",
       " 'boofyr',\n",
       " 'frasers',\n",
       " 'raniakhalek',\n",
       " 'mczehut',\n",
       " 'home',\n",
       " 'kill',\n",
       " 'grandma',\n",
       " 'coronavirus',\n",
       " 'watch',\n",
       " 'frontlin',\n",
       " 'church',\n",
       " 'partner',\n",
       " 'work',\n",
       " 'dure',\n",
       " 'coronaviru',\n",
       " 'pandemic',\n",
       " 'vimeo',\n",
       " 'https',\n",
       " 'bjpvlxo',\n",
       " 'rick',\n",
       " 'pescator',\n",
       " 'sorri',\n",
       " 'friend',\n",
       " 'covid',\n",
       " 'problem',\n",
       " 'covid',\n",
       " 'welcom',\n",
       " 'development',\n",
       " 'respons',\n",
       " 'improv',\n",
       " 'servic',\n",
       " 'health',\n",
       " 'educ',\n",
       " 'alway',\n",
       " 'respons',\n",
       " 'government',\n",
       " 'viru',\n",
       " 'excuse',\n",
       " 'https',\n",
       " 'interesting',\n",
       " 'theori',\n",
       " 'wear',\n",
       " 'https',\n",
       " 'xvhsafjgzj',\n",
       " 'thankyou',\n",
       " 'falkirk',\n",
       " 'herald',\n",
       " 'number',\n",
       " 'covid',\n",
       " 'case',\n",
       " 'alarming',\n",
       " 'warn',\n",
       " 'deputi',\n",
       " 'bujar',\n",
       " 'citizen',\n",
       " 'respect',\n",
       " 'prevent',\n",
       " 'measures',\n",
       " 'today',\n",
       " 'advanc',\n",
       " 'telecommun',\n",
       " 'save',\n",
       " 'mani',\n",
       " 'live',\n",
       " 'bring',\n",
       " 'societi',\n",
       " 'togath',\n",
       " 'time',\n",
       " 'covid',\n",
       " 'crisis',\n",
       " 'greet',\n",
       " 'worldtelecommunicationday',\n",
       " 'pledg',\n",
       " 'welfar',\n",
       " 'mankind',\n",
       " 'hero',\n",
       " 'kerala',\n",
       " 'success',\n",
       " 'handl',\n",
       " 'pandem',\n",
       " 'nipah',\n",
       " 'virus',\n",
       " 'smartest',\n",
       " 'minist',\n",
       " 'india',\n",
       " 'full',\n",
       " 'credit',\n",
       " 'intellig',\n",
       " 'salut',\n",
       " 'success',\n",
       " 'shailajateach',\n",
       " 'https',\n",
       " 'kxgysnet',\n",
       " 'coronavirus',\n",
       " 'thousand',\n",
       " 'forc',\n",
       " 'leav',\n",
       " 'week',\n",
       " 'current',\n",
       " 'home',\n",
       " 'offic',\n",
       " 'guidelin',\n",
       " 'specul',\n",
       " 'made',\n",
       " 'brought',\n",
       " 'pandem',\n",
       " 'india',\n",
       " 'larger',\n",
       " 'extent',\n",
       " 'therefor',\n",
       " 'citi',\n",
       " 'mumbai',\n",
       " 'ahmedabad',\n",
       " 'delhi',\n",
       " 'affect',\n",
       " 'great',\n",
       " 'extent',\n",
       " 'https',\n",
       " 'wqfabcpi',\n",
       " 'juliatint',\n",
       " 'onetoryvoic',\n",
       " 'jamescleverli',\n",
       " 'stupid',\n",
       " 'https',\n",
       " 'qfmhahq',\n",
       " 'belgium',\n",
       " 'europe',\n",
       " 'dozen',\n",
       " 'cuban',\n",
       " 'doctor',\n",
       " 'mexico',\n",
       " 'fight',\n",
       " 'coronavirus',\n",
       " 'report',\n",
       " 'https',\n",
       " 'cmrmpwsfyu',\n",
       " 'sensatez',\n",
       " 'https',\n",
       " 'gsndasyvos',\n",
       " 'wait',\n",
       " 'https',\n",
       " 'kqwvi',\n",
       " 'prev',\n",
       " 'spread',\n",
       " 'covid',\n",
       " 'home',\n",
       " 'sick',\n",
       " 'household',\n",
       " 'member',\n",
       " 'ehsaa',\n",
       " 'telethon',\n",
       " 'live',\n",
       " 'pakistani',\n",
       " 'channels',\n",
       " 'pleas',\n",
       " 'donate',\n",
       " 'latest',\n",
       " 'restaur',\n",
       " 'daili',\n",
       " 'news',\n",
       " 'https',\n",
       " 'jbxgvc',\n",
       " 'nzrugbyheaven',\n",
       " 'artgirl',\n",
       " 'andrea',\n",
       " 'realestatelu',\n",
       " 'covid',\n",
       " 'california',\n",
       " 'case',\n",
       " 'covid',\n",
       " 'today',\n",
       " 'deaths',\n",
       " 'ranjanpanda',\n",
       " 'pmoindia',\n",
       " 'nitiaayog',\n",
       " 'odisha',\n",
       " 'nsitharaman',\n",
       " 'india',\n",
       " 'whiteband',\n",
       " 'food',\n",
       " 'odisha',\n",
       " 'odisha',\n",
       " 'gandhian',\n",
       " 'sufficiency',\n",
       " 'bring',\n",
       " 'villag',\n",
       " 'section',\n",
       " 'growth',\n",
       " 'development',\n",
       " 'india',\n",
       " 'merchandis',\n",
       " 'export',\n",
       " 'plung',\n",
       " 'covid',\n",
       " 'outbreak',\n",
       " 'everyon',\n",
       " 'send',\n",
       " 'coupl',\n",
       " 'buck',\n",
       " 'mitch',\n",
       " 'mcconnell',\n",
       " 'opponent',\n",
       " 'amymcgrathki',\n",
       " 'crush',\n",
       " 'election',\n",
       " 'https',\n",
       " 'ktquj',\n",
       " 'https',\n",
       " 'neitinq',\n",
       " 'morn',\n",
       " 'sycoph',\n",
       " 'https',\n",
       " 'nvgqcwhk',\n",
       " 'coronaviru',\n",
       " 'linger',\n",
       " 'longer',\n",
       " 'previous',\n",
       " 'thought',\n",
       " 'scientist',\n",
       " 'claim',\n",
       " 'news',\n",
       " 'morningmika',\n",
       " 'nygovcuomo',\n",
       " 'room',\n",
       " 'trump-label',\n",
       " 'build',\n",
       " 'hospit',\n",
       " 'beds',\n",
       " 'eubrainwash',\n",
       " 'helena',\n",
       " 'jenni',\n",
       " 'nellytel',\n",
       " 'exact',\n",
       " 'agenda',\n",
       " 'push',\n",
       " 'people',\n",
       " 'great',\n",
       " 'episod',\n",
       " 'propaganda',\n",
       " 'watch',\n",
       " 'ment',\n",
       " 'health',\n",
       " 'awar',\n",
       " 'week',\n",
       " 'begin',\n",
       " 'week',\n",
       " 'theme',\n",
       " 'kindness',\n",
       " 'observerug',\n",
       " 'pleas',\n",
       " 'give',\n",
       " 'chines',\n",
       " 'chanc',\n",
       " 'things',\n",
       " 'penis',\n",
       " 'turtl',\n",
       " 'burnt',\n",
       " 'immediately',\n",
       " 'chines',\n",
       " 'mutat',\n",
       " 'covid',\n",
       " 'ankdasco',\n",
       " 'satapathi',\n",
       " 'nrajabpcl',\n",
       " 'nair',\n",
       " 'jitin',\n",
       " 'swetamishraa',\n",
       " 'nair',\n",
       " 'hena',\n",
       " 'arundeshpande',\n",
       " 'anju',\n",
       " 'purohit',\n",
       " 'ksmann',\n",
       " 'prakasia',\n",
       " 'chitranayal',\n",
       " 'narendramodi',\n",
       " 'majorgauravarya',\n",
       " 'desertfox',\n",
       " 'soniagurnani',\n",
       " 'paraskghelani',\n",
       " 'shitul',\n",
       " 'drdo',\n",
       " 'india',\n",
       " 'makeinindia',\n",
       " 'levinaneythiri',\n",
       " 'savitha',\n",
       " 'theindianmukesh',\n",
       " 'majorpoonia',\n",
       " 'sangitarchopra',\n",
       " 'richa',\n",
       " 'anand',\n",
       " 'modibrigade',\n",
       " 'dhaval',\n",
       " 'lakshmianand',\n",
       " 'onlinerajan',\n",
       " 'hamara',\n",
       " 'desh',\n",
       " 'ramesh',\n",
       " 'gauravmodifi',\n",
       " 'nationalistin',\n",
       " 'meenakshisharan',\n",
       " 'aprnh',\n",
       " 'namoarunima',\n",
       " 'pankaj',\n",
       " 'namo',\n",
       " 'mnam',\n",
       " 'drrajinder',\n",
       " 'abhinandankaul',\n",
       " 'bobbyamit',\n",
       " 'mahendra',\n",
       " 'rajagopalan',\n",
       " 'natura',\n",
       " 'kashyap',\n",
       " 'dauntingsh',\n",
       " 'sukanyaiyer',\n",
       " 'icmr',\n",
       " 'miss',\n",
       " 'news',\n",
       " 'knew',\n",
       " 'didn',\n",
       " 'belong',\n",
       " 'anyon',\n",
       " 'periodt',\n",
       " 'https',\n",
       " 'ryjvug',\n",
       " 'corona',\n",
       " 'viru',\n",
       " 'cardi',\n",
       " 'meet',\n",
       " 'metal',\n",
       " 'narendramodi',\n",
       " 'presid',\n",
       " 'european',\n",
       " 'council',\n",
       " 'charl',\n",
       " 'michel',\n",
       " 'discuss',\n",
       " 'thursday',\n",
       " 'situat',\n",
       " 'aris',\n",
       " 'covid',\n",
       " 'outbreak',\n",
       " 'recogn',\n",
       " 'import',\n",
       " 'region',\n",
       " 'global',\n",
       " 'coordin',\n",
       " 'effect',\n",
       " 'address',\n",
       " 'health',\n",
       " 'econom',\n",
       " 'impact',\n",
       " 'pandemic',\n",
       " 'kudos',\n",
       " 'anoth',\n",
       " 'total',\n",
       " 'lockdown',\n",
       " 'covid',\n",
       " 'case',\n",
       " 'surg',\n",
       " 'https',\n",
       " 'boyoa',\n",
       " 'woooo',\n",
       " 'firstdogonmoon',\n",
       " 'live',\n",
       " 'action',\n",
       " 'cartoon',\n",
       " 'bonu',\n",
       " 'excus',\n",
       " 'ongo',\n",
       " 'word',\n",
       " 'irregardless',\n",
       " 'https',\n",
       " 'axgkbugu',\n",
       " 'cbkkenya',\n",
       " 'data',\n",
       " 'show',\n",
       " 'daili',\n",
       " 'averag',\n",
       " 'm-pesa',\n",
       " 'transact',\n",
       " 'high-valu',\n",
       " 'deal',\n",
       " 'drop',\n",
       " 'apri',\n",
       " 'compar',\n",
       " 'befor',\n",
       " 'kenya',\n",
       " 'announc',\n",
       " 'covid',\n",
       " 'case',\n",
       " 'daili',\n",
       " 'drop',\n",
       " 'businessdaili',\n",
       " 'https',\n",
       " 'acoj',\n",
       " 'alexwickham',\n",
       " 'yeah',\n",
       " 'becaus',\n",
       " 'scienc',\n",
       " 'clearli',\n",
       " 'respons',\n",
       " 'recommend',\n",
       " 'stock',\n",
       " 'found',\n",
       " 'inadequ',\n",
       " 'especi',\n",
       " 'sinc',\n",
       " 'scienc',\n",
       " 'pandem',\n",
       " 'alway',\n",
       " 'threat',\n",
       " 'nation',\n",
       " 'covid',\n",
       " 'criminalneglig',\n",
       " 'borisresign',\n",
       " 'covid',\n",
       " 'disrupt',\n",
       " 'timeline',\n",
       " 'time',\n",
       " 'plan',\n",
       " 'perfect',\n",
       " 'wedding',\n",
       " 'proud',\n",
       " 'friend',\n",
       " 'https',\n",
       " 'idyvabbs',\n",
       " 'covid',\n",
       " 'total',\n",
       " 'number',\n",
       " 'confirm',\n",
       " 'covid',\n",
       " 'case',\n",
       " 'north',\n",
       " 'west',\n",
       " 'increas',\n",
       " 'case',\n",
       " 'previous',\n",
       " 'report',\n",
       " 'cases',\n",
       " 'addit',\n",
       " 'case',\n",
       " 'confirm',\n",
       " 'rustenburg',\n",
       " 'mahikeng',\n",
       " 'matlosana',\n",
       " 'https',\n",
       " 'fpwsodqagp',\n",
       " 'profkarolsikora',\n",
       " 'anshul',\n",
       " 'voic',\n",
       " 'reason',\n",
       " 'coronavirus',\n",
       " 'live',\n",
       " 'counti',\n",
       " 'oregon',\n",
       " 'case',\n",
       " 'death',\n",
       " 'covid',\n",
       " 'state',\n",
       " 'groceri',\n",
       " 'store',\n",
       " 'afternoon',\n",
       " 'dozen',\n",
       " 'custom',\n",
       " 'store',\n",
       " 'fewer',\n",
       " 'half',\n",
       " 'dozen',\n",
       " 'wore',\n",
       " 'masks',\n",
       " 'worker',\n",
       " 'wore',\n",
       " 'masks',\n",
       " 'make',\n",
       " 'workers',\n",
       " 'china',\n",
       " 'behav',\n",
       " 'petti',\n",
       " 'https',\n",
       " 'kgbpsrzpga',\n",
       " 'alexmil',\n",
       " 'myraemacdonald',\n",
       " 'sonaliranad',\n",
       " 'royalfamili',\n",
       " 'usaid',\n",
       " 'narendramodi',\n",
       " 'potu',\n",
       " 'realdonaldtrump',\n",
       " 'barackobama',\n",
       " 'india',\n",
       " 'william',\n",
       " 'satan',\n",
       " 'antichrist',\n",
       " 'thano',\n",
       " 'elizabeth',\n",
       " 'empir',\n",
       " 'covid',\n",
       " 'overhyp',\n",
       " 'media',\n",
       " 'https',\n",
       " 'sjou',\n",
       " 'drzwelimkh',\n",
       " 'ministri',\n",
       " 'check',\n",
       " 'effect',\n",
       " 'network',\n",
       " 'roll',\n",
       " 'symptom',\n",
       " 'covid',\n",
       " 'radiat',\n",
       " 'emit',\n",
       " 'network',\n",
       " 'pleas',\n",
       " 'good',\n",
       " 'morning',\n",
       " 'stronger',\n",
       " 'food',\n",
       " 'shortages',\n",
       " 'thousand',\n",
       " 'australian',\n",
       " 'stuck',\n",
       " 'abroad',\n",
       " 'amid',\n",
       " 'coronaviru',\n",
       " 'plead',\n",
       " 'home',\n",
       " 'guardian',\n",
       " 'https',\n",
       " 'gzsk',\n",
       " 'googlenews',\n",
       " 'european',\n",
       " 'major',\n",
       " 'action',\n",
       " 'alreadi',\n",
       " 'suffer',\n",
       " 'oversupply',\n",
       " 'covid',\n",
       " 'outbreak',\n",
       " 'push',\n",
       " 'energi',\n",
       " 'sector',\n",
       " 'revis',\n",
       " 'busi',\n",
       " 'plan',\n",
       " 'https',\n",
       " 'gtvlbkjr',\n",
       " 'increas',\n",
       " 'month',\n",
       " 'snap',\n",
       " 'applic',\n",
       " 'march',\n",
       " 'listen',\n",
       " 'fractweet',\n",
       " 'aphsa',\n",
       " 'freshebt',\n",
       " 'egpcebt',\n",
       " 'patmlri',\n",
       " 'foodbankwma',\n",
       " 'worcctyfoodbank',\n",
       " 'projectbread',\n",
       " 'bosfoodbank',\n",
       " 'mindyforma',\n",
       " 'childrenshw',\n",
       " 'theopendoorma',\n",
       " 'timgarvinuw',\n",
       " 'sethmnadeau',\n",
       " 'https',\n",
       " 'fnnhlcskkj',\n",
       " 'fake',\n",
       " 'whatsapp',\n",
       " 'video',\n",
       " 'attack',\n",
       " 'doctors',\n",
       " 'covid',\n",
       " 'product',\n",
       " 'search',\n",
       " 'trend',\n",
       " 'amid',\n",
       " 'covid',\n",
       " 'infographic',\n",
       " 'https',\n",
       " 'qjxrpouum',\n",
       " 'https',\n",
       " 'sftd',\n",
       " 'things',\n",
       " 'covid',\n",
       " 'landlord',\n",
       " 'tenant',\n",
       " 'updat',\n",
       " 'https',\n",
       " 'iyvd',\n",
       " 'hlcx',\n",
       " 'https',\n",
       " 'okrimge',\n",
       " 'pelosi',\n",
       " 'thing',\n",
       " 'kennedi',\n",
       " 'center',\n",
       " 'money',\n",
       " 'worthless',\n",
       " 'rhetoric',\n",
       " 'https',\n",
       " 'zbkfnn',\n",
       " 'break',\n",
       " 'indonesia',\n",
       " 'report',\n",
       " 'case',\n",
       " 'death',\n",
       " 'coronavirus',\n",
       " 'rais',\n",
       " 'total',\n",
       " 'case',\n",
       " 'dead',\n",
       " \"gov't\",\n",
       " 'official',\n",
       " 'church',\n",
       " 'china',\n",
       " 'preach',\n",
       " 'patriotism',\n",
       " 'reopen',\n",
       " 'coronaviru',\n",
       " 'https',\n",
       " 'eiko',\n",
       " 'cnalive',\n",
       " 'rsna',\n",
       " 'abstract',\n",
       " 'submiss',\n",
       " 'deadlin',\n",
       " 'days',\n",
       " 'submitting',\n",
       " 'student',\n",
       " 'post',\n",
       " 'wondering',\n",
       " 'realist',\n",
       " 'expect',\n",
       " 'rsna',\n",
       " 'happen',\n",
       " 'peopl',\n",
       " 'entir',\n",
       " 'meet',\n",
       " 'dure',\n",
       " 'winter',\n",
       " 'chicago',\n",
       " 'covid',\n",
       " 'unpredictable',\n",
       " 'healthi',\n",
       " 'symptoms',\n",
       " 'great',\n",
       " 'president',\n",
       " 'https',\n",
       " 'chtck',\n",
       " 'coronaviru',\n",
       " 'israel',\n",
       " 'live',\n",
       " 'ministri',\n",
       " 'netanyahu',\n",
       " 'fulli',\n",
       " 'reopen',\n",
       " 'school',\n",
       " 'https',\n",
       " 'jbdy',\n",
       " 'gentlemen',\n",
       " 'governor',\n",
       " 'hassanalijoho',\n",
       " 'mheshimiwa',\n",
       " 'suleimanshahb',\n",
       " 'fantoman',\n",
       " 'expert',\n",
       " 'coronaviru',\n",
       " 'respons',\n",
       " 'climat',\n",
       " 'change',\n",
       " 'typical',\n",
       " 'assaf',\n",
       " 'member',\n",
       " 'prof',\n",
       " 'glenda',\n",
       " 'gray',\n",
       " 'global',\n",
       " 'fight',\n",
       " 'coronaviru',\n",
       " 'reekado',\n",
       " 'bank',\n",
       " 'gabon',\n",
       " 'govt',\n",
       " 'return',\n",
       " 'home',\n",
       " 'https',\n",
       " 'ncmxzspwqj',\n",
       " 'dear',\n",
       " 'senrickscott',\n",
       " 'health',\n",
       " 'care',\n",
       " 'worker',\n",
       " 'medic',\n",
       " 'suppli',\n",
       " 'covid',\n",
       " 'constituent',\n",
       " 'support',\n",
       " 'heroesact',\n",
       " 'getusppe',\n",
       " 'anoth',\n",
       " 'council',\n",
       " 'school',\n",
       " 'stay',\n",
       " 'shut',\n",
       " 'june',\n",
       " 'teachers',\n",
       " 'coronaviru',\n",
       " 'deepen',\n",
       " 'https',\n",
       " 'osjjc',\n",
       " 'london',\n",
       " 'standard',\n",
       " 'deepli',\n",
       " 'irrespons',\n",
       " 'behaviour',\n",
       " 'https',\n",
       " 'wblwn',\n",
       " 'covid',\n",
       " 'coronaviru',\n",
       " 'lockdown',\n",
       " 'wednesday',\n",
       " 'ikpeazu',\n",
       " 'deputy',\n",
       " 'exco',\n",
       " 'member',\n",
       " 'undergo',\n",
       " 'covid',\n",
       " 'test',\n",
       " 'https',\n",
       " 'lxcw',\n",
       " 'sdcwa',\n",
       " 'realdonaldtrump',\n",
       " 'https',\n",
       " 'wiwggke',\n",
       " 'coronaviru',\n",
       " 'outbreak',\n",
       " 'live',\n",
       " 'updates',\n",
       " 'india',\n",
       " 'regist',\n",
       " 'case',\n",
       " 'record',\n",
       " 'test',\n",
       " 'posit',\n",
       " 'india',\n",
       " 'covid',\n",
       " 'publichealth',\n",
       " 'https',\n",
       " 'unfg',\n",
       " 'coronaviru',\n",
       " 'origin',\n",
       " 'bungl',\n",
       " 'experi',\n",
       " 'wuhan',\n",
       " 'bombshel',\n",
       " 'report',\n",
       " 'claim',\n",
       " 'https',\n",
       " 'oerart',\n",
       " 'coronavirus',\n",
       " 'state',\n",
       " 'reopen',\n",
       " 'ahead',\n",
       " 'white',\n",
       " 'hous',\n",
       " 'guidelines',\n",
       " 'https',\n",
       " 'oqqgpebvcu',\n",
       " 'coronavirus',\n",
       " 'cina',\n",
       " 'shulan',\n",
       " 'nord-est',\n",
       " 'nuovo',\n",
       " 'lockdown',\n",
       " 'wuhan',\n",
       " 'https',\n",
       " 'botvyni',\n",
       " 'https',\n",
       " 'wxoxi',\n",
       " 'ministri',\n",
       " 'confirm',\n",
       " 'covid',\n",
       " 'case',\n",
       " 'sampl',\n",
       " 'truck',\n",
       " 'drivers',\n",
       " 'case',\n",
       " 'covid',\n",
       " 'coronaviru',\n",
       " 'updates',\n",
       " 'covid',\n",
       " 'case',\n",
       " 'chhattisgarh',\n",
       " 'talli',\n",
       " 'patient',\n",
       " 'corona',\n",
       " 'viru',\n",
       " 'reveal',\n",
       " 'chhattisgarh',\n",
       " 'total',\n",
       " 'figur',\n",
       " 'cross',\n",
       " 'https',\n",
       " 'xbusztqlfe',\n",
       " 'nebraska',\n",
       " 'rickett',\n",
       " 'announc',\n",
       " 'state',\n",
       " 'releas',\n",
       " 'data',\n",
       " 'specif',\n",
       " 'number',\n",
       " 'covid',\n",
       " 'case',\n",
       " 'meatpack',\n",
       " 'plants',\n",
       " 'leav',\n",
       " 'public',\n",
       " 'informed',\n",
       " 'safe',\n",
       " 'https',\n",
       " 'nxqzun',\n",
       " 'thing',\n",
       " 'https',\n",
       " 'elwauppzhd',\n",
       " 'heighten',\n",
       " 'sens',\n",
       " 'anxiety',\n",
       " 'polic',\n",
       " 'face',\n",
       " 'covid',\n",
       " 'threat',\n",
       " 'https',\n",
       " 'bvfwa',\n",
       " 'https',\n",
       " 'xpkjm',\n",
       " 'realdonaldtrump',\n",
       " 'https',\n",
       " 'zlllmtd',\n",
       " 'evid',\n",
       " 'good',\n",
       " 'number',\n",
       " 'patients',\n",
       " 'covid',\n",
       " 'coupl',\n",
       " 'crap',\n",
       " 'week',\n",
       " 'back',\n",
       " 'normal',\n",
       " 'nasty',\n",
       " 'worri',\n",
       " 'disease',\n",
       " 'heck',\n",
       " 'catch',\n",
       " 'https',\n",
       " 'hxdwy',\n",
       " 'pyua',\n",
       " 'seanhann',\n",
       " 'oregon',\n",
       " 'liber',\n",
       " 'honk',\n",
       " 'governor',\n",
       " 'spend',\n",
       " 'time',\n",
       " 'https',\n",
       " 'prpoptrwoo',\n",
       " 'refile-dubai',\n",
       " 'develop',\n",
       " 'nakheel',\n",
       " 'slash',\n",
       " 'salari',\n",
       " 'coronaviru',\n",
       " 'crisi',\n",
       " 'https',\n",
       " 'qbieq',\n",
       " 'kaqr',\n",
       " 'medic',\n",
       " 'covid',\n",
       " 'doyin',\n",
       " 'okup',\n",
       " 'reveal',\n",
       " 'https',\n",
       " 'sidlgu',\n",
       " 'hope',\n",
       " 'offici',\n",
       " 'deni',\n",
       " 'claim',\n",
       " 'intellig',\n",
       " 'knew',\n",
       " 'outbreak',\n",
       " 'china',\n",
       " 'earli',\n",
       " 'actual',\n",
       " 'true',\n",
       " 'becaus',\n",
       " 'make',\n",
       " 'befor',\n",
       " 'china',\n",
       " 'offici',\n",
       " 'olanrefront',\n",
       " 'pleas',\n",
       " 'retweet',\n",
       " 'video',\n",
       " 'clip',\n",
       " 'push',\n",
       " 'singl',\n",
       " 'call',\n",
       " 'lockdown',\n",
       " 'leon',\n",
       " 'full',\n",
       " 'song',\n",
       " 'link',\n",
       " 'solomonyu',\n",
       " 'past',\n",
       " 'ccpchina',\n",
       " 'blind',\n",
       " ...]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Regex to indetify for tokenization \n",
    "# Applying condition for length greater than 3 and lower bound \n",
    "\n",
    "tweets_tokenized_F={}\n",
    "tokenizer_F=RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "raw_text=\"\" \n",
    "for item2 in trimmed_tweets_F:\n",
    "    raw_text=raw_text+str(item2)\n",
    "    raw_text=raw_text.lower()\n",
    "    unigram_tokens_F=tokenizer_F.tokenize(raw_text)\n",
    "    unigram_tokens_F=[value for index,value in enumerate(unigram_tokens_F) if len(value)>3 and value.lower() not in stopwords]\n",
    "unigram_tokens_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['martmabrey',\n",
       " 'inkstainedretch',\n",
       " 'totally',\n",
       " 'democrat',\n",
       " 'legal',\n",
       " 'protect',\n",
       " 'busi',\n",
       " 'open',\n",
       " 'earli',\n",
       " 'babbl',\n",
       " 'idiot',\n",
       " 'file',\n",
       " 'class',\n",
       " 'action',\n",
       " 'trump',\n",
       " 'coronaviru',\n",
       " 'https',\n",
       " 'hcgg',\n",
       " 'zmbxb',\n",
       " 'https',\n",
       " 'sweden',\n",
       " 'found',\n",
       " 'solut',\n",
       " 'coronavirus',\n",
       " 'https',\n",
       " 'motspldebp',\n",
       " 'coronavirus',\n",
       " 'racism',\n",
       " 'injustice',\n",
       " 'save',\n",
       " 'york',\n",
       " 'times',\n",
       " 'struggl',\n",
       " 'understand',\n",
       " 'black',\n",
       " 'peopl',\n",
       " 'riot',\n",
       " 'offer',\n",
       " 'altern',\n",
       " 'peopl',\n",
       " 'lifetim',\n",
       " 'rage',\n",
       " 'disempower',\n",
       " 'injustice',\n",
       " 'https',\n",
       " 'htdevv',\n",
       " 'headlin',\n",
       " 'florida',\n",
       " 'leav',\n",
       " 'doubt',\n",
       " 'state',\n",
       " 'becom',\n",
       " 'coronaviru',\n",
       " 'hotspot',\n",
       " 'florida',\n",
       " 'covid',\n",
       " 'case',\n",
       " 'surg',\n",
       " 'nearli',\n",
       " 'desanti',\n",
       " 'resist',\n",
       " 'statewid',\n",
       " 'restrict',\n",
       " 'child',\n",
       " 'autism',\n",
       " 'spectrum',\n",
       " 'disord',\n",
       " 'physic',\n",
       " 'activ',\n",
       " 'reduc',\n",
       " 'stress',\n",
       " 'home',\n",
       " 'import',\n",
       " 'debate',\n",
       " 'meet',\n",
       " 'goal',\n",
       " 'https',\n",
       " 'zrula',\n",
       " 'exempt',\n",
       " 'countri',\n",
       " 'quarantin',\n",
       " 'https',\n",
       " 'hbebut',\n",
       " 'wang',\n",
       " 'sinc',\n",
       " 'covid',\n",
       " 'started',\n",
       " 'member',\n",
       " 'state',\n",
       " 'shanghai',\n",
       " 'spirit',\n",
       " 'contribut',\n",
       " 'share',\n",
       " 'global',\n",
       " 'cooper',\n",
       " 'concret',\n",
       " 'ways',\n",
       " 'convinc',\n",
       " 'strong',\n",
       " 'leadership',\n",
       " 'leaders',\n",
       " 'ultim',\n",
       " 'victori',\n",
       " 'virus',\n",
       " 'https',\n",
       " 'xjjidi',\n",
       " 'mention',\n",
       " 'tattoo',\n",
       " 'shop',\n",
       " 'reopening',\n",
       " 'artist',\n",
       " 'follow',\n",
       " 'link',\n",
       " 'reopen',\n",
       " 'chanc',\n",
       " 'answer',\n",
       " 'https',\n",
       " 'yfxnnjd',\n",
       " 'funni',\n",
       " 'accur',\n",
       " 'https',\n",
       " 'siqn',\n",
       " 'exbcz',\n",
       " 'deadlin',\n",
       " 'entrant',\n",
       " 'coronaviru',\n",
       " 'retent',\n",
       " 'scheme',\n",
       " 'furlough',\n",
       " 'days',\n",
       " 'reach',\n",
       " 'agil',\n",
       " 'requir',\n",
       " 'advic',\n",
       " 'support',\n",
       " 'agilehrconsultingltd',\n",
       " 'furlough',\n",
       " 'hradvic',\n",
       " 'hrsupport',\n",
       " 'covid',\n",
       " 'zealand',\n",
       " 'flag',\n",
       " 'four-day',\n",
       " 'work',\n",
       " 'week',\n",
       " 'boost',\n",
       " 'shutter',\n",
       " 'economy',\n",
       " 'coronaviru',\n",
       " 'https',\n",
       " 'unicef',\n",
       " 'warn',\n",
       " 'lockdown',\n",
       " 'kill',\n",
       " 'covid',\n",
       " 'develop',\n",
       " 'countri',\n",
       " 'https',\n",
       " 'svdn',\n",
       " 'appar',\n",
       " 'unpreced',\n",
       " 'move',\n",
       " 'treasuri',\n",
       " 'includ',\n",
       " 'trump',\n",
       " 'coronaviru',\n",
       " 'stimulu',\n",
       " 'check',\n",
       " 'https',\n",
       " 'edknnnq',\n",
       " 'mich',\n",
       " 'governor',\n",
       " 'court',\n",
       " 'fight',\n",
       " 'coronaviru',\n",
       " 'stay-hom',\n",
       " 'order',\n",
       " 'https',\n",
       " 'global',\n",
       " 'epidem',\n",
       " 'froze',\n",
       " 'world',\n",
       " 'lead',\n",
       " 'direct',\n",
       " 'broaden',\n",
       " 'view',\n",
       " 'friendli',\n",
       " 'harmoni',\n",
       " 'attitud',\n",
       " 'natur',\n",
       " 'shift',\n",
       " 'previou',\n",
       " 'realiti',\n",
       " 'total',\n",
       " 'loss',\n",
       " 'transform',\n",
       " 'total',\n",
       " 'gain',\n",
       " 'balanc',\n",
       " 'world',\n",
       " 'covid',\n",
       " 'corona',\n",
       " 'https',\n",
       " 'advobarryroux',\n",
       " 'minist',\n",
       " 'month',\n",
       " 'learnt',\n",
       " 'neighborhood',\n",
       " 'shoprit',\n",
       " 'employe',\n",
       " 'alreadi',\n",
       " 'suffering',\n",
       " 'import',\n",
       " 'time',\n",
       " 'despit',\n",
       " 'israel',\n",
       " 'palestinian',\n",
       " 'authority',\n",
       " 'hear',\n",
       " 'incit',\n",
       " 'comment',\n",
       " 'palestinian',\n",
       " 'prime',\n",
       " 'minister',\n",
       " 'accus',\n",
       " 'soldier',\n",
       " 'spread',\n",
       " 'coronaviru',\n",
       " 'dannydanon',\n",
       " 'stil',\n",
       " 'raga',\n",
       " 'india',\n",
       " 'prepar',\n",
       " 'readi',\n",
       " 'face',\n",
       " 'foxredresili',\n",
       " 'itdr',\n",
       " 'feed',\n",
       " 'stay',\n",
       " 'resili',\n",
       " 'amidst',\n",
       " 'covid',\n",
       " 'atlant',\n",
       " 'hurrican',\n",
       " 'season',\n",
       " 'https',\n",
       " 'eidvrogthz',\n",
       " 'corcoran',\n",
       " 'odisha',\n",
       " 'cadr',\n",
       " 'coupl',\n",
       " 'chip',\n",
       " 'fight',\n",
       " 'coronaviru',\n",
       " 'pandem',\n",
       " 'https',\n",
       " 'mppcveoe',\n",
       " 'https',\n",
       " 'ddetjfmoa',\n",
       " 'angie',\n",
       " 'rasmussen',\n",
       " 'thought',\n",
       " 'time',\n",
       " 'patience',\n",
       " 'https',\n",
       " 'coronaviru',\n",
       " 'unfortun',\n",
       " 'gave',\n",
       " 'mother',\n",
       " 'beat',\n",
       " \"she'll\",\n",
       " 'beat',\n",
       " 'posit',\n",
       " 'vibe',\n",
       " 'onli',\n",
       " 'member',\n",
       " 'swig',\n",
       " 'financ',\n",
       " 'signpoint',\n",
       " 'import',\n",
       " 'onlin',\n",
       " 'support',\n",
       " 'offer',\n",
       " 'struggl',\n",
       " 'mental',\n",
       " 'current',\n",
       " 'circumst',\n",
       " 'https',\n",
       " 'kayfxc',\n",
       " 'hous',\n",
       " 'grey',\n",
       " 'grey',\n",
       " 'jajajajajaj',\n",
       " 'https',\n",
       " 'betjz',\n",
       " 'urgent',\n",
       " 'dream',\n",
       " 'speak',\n",
       " 'legal',\n",
       " 'induc',\n",
       " 'abort',\n",
       " 'report',\n",
       " 'pollut',\n",
       " 'worst',\n",
       " 'possibl',\n",
       " 'link',\n",
       " 'covid',\n",
       " 'death',\n",
       " 'https',\n",
       " 'xbcg',\n",
       " 'npnas',\n",
       " 'mani',\n",
       " 'state',\n",
       " 'explor',\n",
       " 'hous',\n",
       " 'arrest',\n",
       " 'technolog',\n",
       " 'covid',\n",
       " 'patient',\n",
       " 'home',\n",
       " 'https',\n",
       " 'xrftqmlax',\n",
       " 'coronavirus',\n",
       " 'baltic',\n",
       " 'state',\n",
       " 'open',\n",
       " 'pandem',\n",
       " 'travel',\n",
       " 'bubble',\n",
       " 'unifi',\n",
       " 'command',\n",
       " 'confirm',\n",
       " 'case',\n",
       " 'covid',\n",
       " 'panhandle',\n",
       " 'activ',\n",
       " 'case',\n",
       " 'recoveries',\n",
       " 'good',\n",
       " 'open',\n",
       " 'state',\n",
       " 'result',\n",
       " 'infect',\n",
       " 'fatalities',\n",
       " 'pandem',\n",
       " 'live',\n",
       " 'claim',\n",
       " 'show',\n",
       " 'deadliness',\n",
       " 'peopl',\n",
       " 'make',\n",
       " 'light',\n",
       " 'becaus',\n",
       " 'alreadi',\n",
       " 'test',\n",
       " 'six-metr',\n",
       " 'social',\n",
       " 'distanc',\n",
       " 'rule',\n",
       " 'oper',\n",
       " 'royal',\n",
       " 'stoke',\n",
       " 'coronaviru',\n",
       " 'patient',\n",
       " 'remain',\n",
       " 'ward',\n",
       " 'stoke-on',\n",
       " 'live',\n",
       " 'https',\n",
       " 'yivpg',\n",
       " 'offer',\n",
       " 'voluntari',\n",
       " 'layoff',\n",
       " 'employe',\n",
       " 'amid',\n",
       " 'coronaviru',\n",
       " 'fallout',\n",
       " 'offer',\n",
       " 'made',\n",
       " 'today',\n",
       " 'https',\n",
       " 'cfyu',\n",
       " 'vrnyf',\n",
       " 'https',\n",
       " 'theodor',\n",
       " 'roosevelt',\n",
       " 'entir',\n",
       " 'crew',\n",
       " 'test',\n",
       " 'coronavirus',\n",
       " 'positive',\n",
       " 'offici',\n",
       " 'https',\n",
       " 'ivooe',\n",
       " 'valu',\n",
       " 'capabl',\n",
       " 'evalu',\n",
       " 'modi',\n",
       " 'yogi',\n",
       " 'govern',\n",
       " 'https',\n",
       " 'gxyggkrjpn',\n",
       " 'theshiftpr',\n",
       " 'ject',\n",
       " 'nytim',\n",
       " 'washingtonpost',\n",
       " 'guardianeco',\n",
       " 'nbcnew',\n",
       " 'scifri',\n",
       " 'covid',\n",
       " 'https',\n",
       " 'rigkgmgird',\n",
       " 'human',\n",
       " 'vanish',\n",
       " 'life',\n",
       " 'earth',\n",
       " 'flourish',\n",
       " 'earlier',\n",
       " 'sadhguru',\n",
       " 'natur',\n",
       " 'prove',\n",
       " 'coronavirus',\n",
       " 'breaking',\n",
       " 'bombay',\n",
       " 'airlin',\n",
       " 'middl',\n",
       " 'seat',\n",
       " 'occupi',\n",
       " 'expert',\n",
       " 'committe',\n",
       " 'mere',\n",
       " 'touch',\n",
       " 'covid',\n",
       " 'infect',\n",
       " 'person',\n",
       " 'transmit',\n",
       " 'viru',\n",
       " 'https',\n",
       " 'mtty',\n",
       " 'posit',\n",
       " 'aftereffect',\n",
       " 'coronaviru',\n",
       " 'veri',\n",
       " 'appetit',\n",
       " 'real',\n",
       " 'viru',\n",
       " 'racism',\n",
       " 'sinc',\n",
       " 'coronaviru',\n",
       " 'began',\n",
       " 'pandem',\n",
       " 'shift',\n",
       " 'epic',\n",
       " 'proportions',\n",
       " 'oldtakesexpos',\n",
       " 'eveyrth',\n",
       " 'theyll',\n",
       " 'covid',\n",
       " 'prevent',\n",
       " 'happening',\n",
       " 'firm',\n",
       " 'slash',\n",
       " 'entry-level',\n",
       " 'quarter',\n",
       " 'coronaviru',\n",
       " 'https',\n",
       " 'zkedeym',\n",
       " 'paapa',\n",
       " 'citi',\n",
       " 'haven',\n",
       " 'decid',\n",
       " 'comparison',\n",
       " 'haven',\n",
       " 'bawunia',\n",
       " 'compar',\n",
       " 'covid',\n",
       " 'dumsor',\n",
       " 'write',\n",
       " 'hurri',\n",
       " 'attack',\n",
       " 'forget',\n",
       " 'understand',\n",
       " 'premis',\n",
       " 'argument',\n",
       " 'attacking',\n",
       " 'veri',\n",
       " 'servic',\n",
       " 'answer',\n",
       " 'unansw',\n",
       " 'question',\n",
       " 'coronaviru',\n",
       " 'https',\n",
       " 'bbuve',\n",
       " 'coverag',\n",
       " 'relev',\n",
       " 'badpharma',\n",
       " 'thread',\n",
       " 'time',\n",
       " 'motherjon',\n",
       " 'podcast',\n",
       " 'talk',\n",
       " 'dodgi',\n",
       " 'covid',\n",
       " 'pharmaceutical-sponsor',\n",
       " 'trial',\n",
       " 'remdesivir',\n",
       " 'trial',\n",
       " 'publish',\n",
       " 'nejm',\n",
       " 'ibuprofen',\n",
       " 'scare',\n",
       " 'diagnost',\n",
       " 'test',\n",
       " 'fail',\n",
       " 'worth',\n",
       " 'listen',\n",
       " 'https',\n",
       " 'latest',\n",
       " 'contact-cent',\n",
       " 'daily',\n",
       " 'https',\n",
       " 'lasvfut',\n",
       " 'secur',\n",
       " 'covid',\n",
       " 'mcfarland',\n",
       " 'surpris',\n",
       " 'point',\n",
       " 'https',\n",
       " 'xwoeeu',\n",
       " 'belgium',\n",
       " 'europe',\n",
       " 'highest',\n",
       " 'covid',\n",
       " 'death-rat',\n",
       " 'https',\n",
       " 'dptqaukgwj',\n",
       " 'https',\n",
       " 'sqkfdvp',\n",
       " 'hahahahahahahahahahaha',\n",
       " 'https',\n",
       " 'lpukenij',\n",
       " 'coronaviru',\n",
       " 'entir',\n",
       " 'worth',\n",
       " 'return',\n",
       " 'milo',\n",
       " 'kerrigan',\n",
       " 'shaunmicallef',\n",
       " 'madashel',\n",
       " 'https',\n",
       " 'rbrmbq',\n",
       " 'offer',\n",
       " 'voluntari',\n",
       " 'layoff',\n",
       " 'employe',\n",
       " 'tide',\n",
       " 'coronaviru',\n",
       " 'fallout',\n",
       " 'https',\n",
       " 'pstrdmy',\n",
       " 'https',\n",
       " 'wzgep',\n",
       " 'india',\n",
       " 'reopen',\n",
       " 'mani',\n",
       " 'temples',\n",
       " 'mall',\n",
       " 'restaur',\n",
       " 'despit',\n",
       " 'grow',\n",
       " 'number',\n",
       " 'coronaviru',\n",
       " 'case',\n",
       " 'https',\n",
       " 'epox',\n",
       " 'studi',\n",
       " 'treatment',\n",
       " 'outcom',\n",
       " 'critic',\n",
       " 'patient',\n",
       " 'covid',\n",
       " 'status',\n",
       " 'recruiting',\n",
       " 'townsouthhadley',\n",
       " 'schoolcommittee',\n",
       " 'work',\n",
       " 'educ',\n",
       " 'association',\n",
       " 'approv',\n",
       " 'paidadministrativeleav',\n",
       " 'full',\n",
       " 'benefit',\n",
       " 'teacher',\n",
       " 'district',\n",
       " 'employe',\n",
       " 'place',\n",
       " 'mandatori',\n",
       " 'quarantin',\n",
       " 'result',\n",
       " 'coronaviru',\n",
       " 'outbreak',\n",
       " 'https',\n",
       " 'ikcihkqbdc',\n",
       " 'yr-old',\n",
       " 'test',\n",
       " 'posit',\n",
       " 'coronaviru',\n",
       " 'kenya',\n",
       " 'number',\n",
       " 'case',\n",
       " 'rise',\n",
       " 'seriou',\n",
       " 'guys',\n",
       " 'everyon',\n",
       " 'higher',\n",
       " 'risk',\n",
       " 'viru',\n",
       " 'irrespect',\n",
       " 'stay',\n",
       " 'safe',\n",
       " 'alway',\n",
       " 'follow',\n",
       " 'govern',\n",
       " 'directives',\n",
       " 'cuomo',\n",
       " 'quest',\n",
       " 'time',\n",
       " 'drama',\n",
       " 'https',\n",
       " 'dhzbgkf',\n",
       " 'coronaviru',\n",
       " 'infest',\n",
       " 'medic',\n",
       " 'arriv',\n",
       " 'china',\n",
       " 'treat',\n",
       " 'mbuhari',\n",
       " 'immedi',\n",
       " 'famili',\n",
       " 'https',\n",
       " 'fzmjhtsctr',\n",
       " 'pleas',\n",
       " 'vote',\n",
       " 'retweet',\n",
       " 'poll',\n",
       " 'world',\n",
       " 'earlymodern',\n",
       " 'dutch',\n",
       " 'joke',\n",
       " 'gabl',\n",
       " 'stone',\n",
       " 'dordrecht',\n",
       " 'world',\n",
       " 'eend',\n",
       " 'duck',\n",
       " 'coronaviru',\n",
       " 'bizar',\n",
       " 'sayno',\n",
       " 'duck',\n",
       " 'https',\n",
       " 'https',\n",
       " 'dejbohtezz',\n",
       " 'zealand',\n",
       " 'offici',\n",
       " 'report',\n",
       " 'current',\n",
       " 'coronaviru',\n",
       " 'patient',\n",
       " 'country',\n",
       " 'hospitals',\n",
       " 'bitcoin',\n",
       " 'safe-haven',\n",
       " 'asset',\n",
       " 'time',\n",
       " 'market',\n",
       " 'instabl',\n",
       " 'midst',\n",
       " 'coronavirus',\n",
       " 'https',\n",
       " 'dsxadicgin',\n",
       " 'thes',\n",
       " 'tech',\n",
       " 'compani',\n",
       " 'provid',\n",
       " 'free',\n",
       " 'remot',\n",
       " 'work',\n",
       " 'tool',\n",
       " 'dure',\n",
       " 'coronaviru',\n",
       " 'outbreak',\n",
       " 'governor',\n",
       " 'https',\n",
       " 'zedseselja',\n",
       " 'https',\n",
       " 'hrzfww',\n",
       " 'canberratimes',\n",
       " 'back',\n",
       " 'climat',\n",
       " 'crisi',\n",
       " 'hoax',\n",
       " 'phys',\n",
       " 'distanc',\n",
       " 'hack',\n",
       " 'number',\n",
       " 'coronaviru',\n",
       " 'death',\n",
       " 'toll',\n",
       " 'rise',\n",
       " 'past',\n",
       " 'hour',\n",
       " 'time',\n",
       " 'india',\n",
       " 'https',\n",
       " 'fhvdw',\n",
       " 'bptl',\n",
       " 'ecuador',\n",
       " 'pro-lif',\n",
       " 'group',\n",
       " 'protest',\n",
       " 'abort',\n",
       " 'condit',\n",
       " 'coronaviru',\n",
       " 'https',\n",
       " 'hbml',\n",
       " 'https',\n",
       " 'gpmskw',\n",
       " 'coronaviru',\n",
       " 'pandem',\n",
       " 'moral',\n",
       " 'test',\n",
       " 'psycholog',\n",
       " 'today',\n",
       " 'https',\n",
       " 'nhqtnum',\n",
       " 'absolut',\n",
       " 'idiot',\n",
       " 'donkey',\n",
       " 'welcom',\n",
       " 'eros',\n",
       " 'freedom',\n",
       " 'surli',\n",
       " 'thickest',\n",
       " 'thick',\n",
       " 'https',\n",
       " 'aged',\n",
       " 'gxido',\n",
       " 'sever',\n",
       " 'milan',\n",
       " 'player',\n",
       " 'affect',\n",
       " 'coronaviru',\n",
       " 'https',\n",
       " 'lefbbsvft',\n",
       " 'author',\n",
       " 'surveil',\n",
       " 'capit',\n",
       " 'surprisingli',\n",
       " 'optimist',\n",
       " 'post-covid',\n",
       " 'world',\n",
       " 'https',\n",
       " 'ukkm',\n",
       " 'corona',\n",
       " 'heat',\n",
       " 'kill',\n",
       " 'covid',\n",
       " 'lying',\n",
       " 'crying',\n",
       " 'lier',\n",
       " 'cost',\n",
       " 'lives',\n",
       " 'sensusancollin',\n",
       " 'stern',\n",
       " 'letters',\n",
       " 'stern',\n",
       " 'leters',\n",
       " 'definit',\n",
       " 'learn',\n",
       " 'lesson',\n",
       " 'nice',\n",
       " 'https',\n",
       " 'mnft',\n",
       " 'https',\n",
       " 'zbgodnleta',\n",
       " 'great',\n",
       " 'actor',\n",
       " 'movi',\n",
       " 'contagion',\n",
       " 'repris',\n",
       " 'role',\n",
       " 'give',\n",
       " 'advic',\n",
       " 'covid',\n",
       " 'conjunct',\n",
       " 'columbia',\n",
       " 'https',\n",
       " 'fuotggrbh',\n",
       " 'controlthecontagion',\n",
       " 'emot',\n",
       " 'landscap',\n",
       " 'chang',\n",
       " 'amidst',\n",
       " 'coronavirus',\n",
       " 'accord',\n",
       " 'expert',\n",
       " 'https',\n",
       " 'covid',\n",
       " 'pandem',\n",
       " 'shatter',\n",
       " 'studi',\n",
       " 'abroad',\n",
       " 'dreams',\n",
       " 'article',\n",
       " 'seropreval',\n",
       " 'immunoglobulin',\n",
       " 'antibodi',\n",
       " 'sars-cov',\n",
       " 'china',\n",
       " 'https',\n",
       " 'coronaviru',\n",
       " 'covid',\n",
       " 'ncov',\n",
       " 'gdad',\n",
       " 'religi',\n",
       " 'institut',\n",
       " 'undermin',\n",
       " 'physic',\n",
       " 'well-b',\n",
       " 'peopl',\n",
       " 'world',\n",
       " 'covid',\n",
       " 'seriou',\n",
       " 'viru',\n",
       " 'make',\n",
       " 'incompet',\n",
       " 'carri',\n",
       " 'film',\n",
       " 'bori',\n",
       " 'kenneth',\n",
       " 'william',\n",
       " 'https',\n",
       " 'lbhs',\n",
       " 'germany',\n",
       " 'restrict',\n",
       " 'pandem',\n",
       " 'final',\n",
       " 'loosen',\n",
       " 'back',\n",
       " 'normal',\n",
       " 'exercis',\n",
       " 'routine',\n",
       " 'month',\n",
       " 'proper',\n",
       " 'workout',\n",
       " 'wait',\n",
       " 'start',\n",
       " 'exercis',\n",
       " 'covid',\n",
       " 'https',\n",
       " 'xqwl',\n",
       " 'feno',\n",
       " 'shashitharoor',\n",
       " 'watch',\n",
       " 'https',\n",
       " 'lnkwebyto',\n",
       " \"we'r\",\n",
       " 'internet',\n",
       " 'shelter',\n",
       " 'home',\n",
       " 'download',\n",
       " 'aarogya',\n",
       " 'setu',\n",
       " 'fight',\n",
       " 'covid',\n",
       " 'assess',\n",
       " 'contact',\n",
       " 'tracing',\n",
       " 'dissemin',\n",
       " 'inform',\n",
       " 'spread',\n",
       " 'covid',\n",
       " 'latest',\n",
       " 'cambodg',\n",
       " 'express',\n",
       " 'https',\n",
       " 'voakhmer',\n",
       " 'coronaviru',\n",
       " 'asean',\n",
       " 'covid',\n",
       " 'content',\n",
       " 'https',\n",
       " 'uuvxtcckj',\n",
       " 'respons',\n",
       " 'discrep',\n",
       " 'uniti',\n",
       " 'harmoni',\n",
       " 'confirm',\n",
       " 'doubt',\n",
       " 'believ',\n",
       " 'covid',\n",
       " 'hoax',\n",
       " 'https',\n",
       " 'vmsnjsw',\n",
       " 'warning',\n",
       " 'algonquin',\n",
       " 'colleg',\n",
       " 'cancel',\n",
       " 'event',\n",
       " 'june',\n",
       " 'covid',\n",
       " 'pandem',\n",
       " 'https',\n",
       " 'mfbw',\n",
       " 'ycamk',\n",
       " 'coronaviru',\n",
       " 'pandemic',\n",
       " 'increas',\n",
       " 'respons',\n",
       " 'unpaid',\n",
       " 'famili',\n",
       " 'carers',\n",
       " 'head',\n",
       " 'carer',\n",
       " 'projects',\n",
       " 'kelli',\n",
       " 'daubney',\n",
       " 'challeng',\n",
       " 'carer',\n",
       " 'face',\n",
       " 'change',\n",
       " 'post',\n",
       " 'https',\n",
       " 'mord',\n",
       " 'carersweek',\n",
       " 'doyoucare',\n",
       " 'coronaviru',\n",
       " 'live',\n",
       " 'updates',\n",
       " 'summit',\n",
       " 'postponed',\n",
       " 'link',\n",
       " 'poverti',\n",
       " 'coronaviru',\n",
       " 'death',\n",
       " 'scotland',\n",
       " 'laid',\n",
       " 'bare',\n",
       " 'shock',\n",
       " 'figur',\n",
       " 'https',\n",
       " 'tizel',\n",
       " 'yxwk',\n",
       " 'interact',\n",
       " 'strateg',\n",
       " 'ahead',\n",
       " 'tackl',\n",
       " 'covid',\n",
       " 'https',\n",
       " 'xlazjdcti',\n",
       " 'case',\n",
       " 'activ',\n",
       " 'case',\n",
       " 'ncdcgov',\n",
       " 'combin',\n",
       " 'activ',\n",
       " 'case',\n",
       " 'total',\n",
       " 'case',\n",
       " 'https',\n",
       " 'mclrs',\n",
       " 'gwii',\n",
       " 'nextdoor',\n",
       " 'featur',\n",
       " 'neighbor',\n",
       " 'dure',\n",
       " 'coronaviru',\n",
       " 'crisis',\n",
       " 'cours',\n",
       " 'troop',\n",
       " 'home',\n",
       " 'elect',\n",
       " 'noth',\n",
       " 'major',\n",
       " 'give',\n",
       " 'fuck',\n",
       " 'blew',\n",
       " 'current',\n",
       " 'covid',\n",
       " 'pandem',\n",
       " 'wipe',\n",
       " 'so-cal',\n",
       " 'boom',\n",
       " 'economy',\n",
       " 'supposedli',\n",
       " 'built',\n",
       " 'fuck',\n",
       " 'trump',\n",
       " 'fuckem',\n",
       " 'https',\n",
       " ...]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Regex to indetify for tokenization \n",
    "# Applying condition for length greater than 3 and lower bound \n",
    "\n",
    "tweets_tokenized_DS={}\n",
    "tokenizer_DS=RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "raw_text=\"\" \n",
    "for item2 in trimmed_tweets_DS:\n",
    "    raw_text=raw_text+str(item2)\n",
    "    raw_text=raw_text.lower()\n",
    "    unigram_tokens_DS=tokenizer_DS.tokenize(raw_text)\n",
    "    unigram_tokens_DS=[value for index,value in enumerate(unigram_tokens_DS) if len(value)>3 and value.lower() not in stopwords]\n",
    "unigram_tokens_DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing vocabulary and their number of occurances in a dictionary\n",
    "vocab_IT=list(set(unigram_tokens_IT))\n",
    "count_IT=[]\n",
    "for j in range(len(vocab_IT)):\n",
    "    count_IT.append(unigram_tokens_IT.count(vocab_IT[j]))\n",
    "IT=dict(zip(unigram_tokens_IT,count_IT))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing vocabulary and their number of occurances in a dictionary\n",
    "vocab_F=list(set(unigram_tokens_F))\n",
    "count_F=[]\n",
    "for j in range(len(vocab_F)):\n",
    "    count_F.append(unigram_tokens_F.count(vocab_F[j]))\n",
    "F=dict(zip(unigram_tokens_IT,count_F))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing vocabulary and their number of occurances in a dictionary\n",
    "vocab_DS=list(set(unigram_tokens_DS))\n",
    "count_DS=[]\n",
    "for j in range(len(vocab_DS)):\n",
    "    count_DS.append(unigram_tokens_DS.count(vocab_DS[j]))\n",
    "DS=dict(zip(unigram_tokens_DS,count_DS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using counter to find most occuring tokens in Data Science\n",
    "DS_Counter = Counter(DS) \n",
    "DS_Uni=DS_Counter.most_common(100)\n",
    "\n",
    "#Using counter to find most occuring tokens in IT\n",
    "IT_Counter = Counter(IT) \n",
    "IT_Uni=IT_Counter.most_common(100)\n",
    "\n",
    "#Using counter to find most occuring tokens in Finance \n",
    "F_Counter = Counter(F) \n",
    "F_Uni=F_Counter.most_common(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining tokens together\n",
    "unigram_tokens=str('IT: '+str(IT_Uni) + '\\nFinance:' + str(F_Uni) + '\\nData Science:'+str(DS_Uni))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Save the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"100uni.txt\", \"w\")\n",
    "text_file.write(unigram_tokens)\n",
    "text_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<br>\n",
    "    \n",
    "## Task 3 - DATA INTEGRATION  <a class=\"anchor\" name=\"task1\">\n",
    "    \n",
    "<br>\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Reading the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading pdf file\n",
    "cwd = os.getcwd()\n",
    "shopping_centers=tabula.read_pdf('shopingcenters.pdf',pages=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading zip file\n",
    "zipdata = ZipFile('gtfs.zip', 'r')\n",
    "files = [name for name in zipdata.namelist() if name.endswith('.zip')]\n",
    "# Getting inside zip file and extracting folders for 1-10\n",
    "\n",
    "df=[]\n",
    "for file in files:\n",
    "    f = file\n",
    "    z = zipfile.ZipFile(f, \"r\")\n",
    "    zinfo = z.namelist()\n",
    "    for name in zinfo:\n",
    "        with z.open(name) as f1:\n",
    "            df.append( pd.read_csv(f1))\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting zip file folder data combined, to seperate dataframes \n",
    "agency_df=pd.DataFrame()\n",
    "route_df=pd.DataFrame()\n",
    "trips_df=pd.DataFrame()\n",
    "stops_df=pd.DataFrame()\n",
    "calendar_df=pd.DataFrame()\n",
    "calendar_dates_df=pd.DataFrame()\n",
    "shapes_df=pd.DataFrame()\n",
    "stop_times_df=pd.DataFrame()\n",
    "\n",
    "agency_df=pd.concat([df[0],df[8],df[16],df[24],df[32],df[40],df[48],df[56],df[64],df[72]], ignore_index=True)\n",
    "route_df=pd.concat([df[1],df[9],df[17],df[25],df[33],df[41],df[49],df[57],df[65],df[73]], ignore_index=True)\n",
    "trips_df=pd.concat([df[2],df[10],df[18],df[26],df[34],df[42],df[50],df[58],df[66],df[74]], ignore_index=True)\n",
    "stops_df=pd.concat([df[3],df[11],df[19],df[27],df[35],df[43],df[51],df[59],df[67],df[75]], ignore_index=True)\n",
    "calendar_df=pd.concat([df[4],df[12],df[20],df[28],df[36],df[44],df[52],df[60],df[68],df[76]], ignore_index=True)\n",
    "calendar_dates_df=pd.concat([df[5],df[13],df[21],df[29],df[37],df[45],df[53],df[61],df[69],df[77]], ignore_index=True)\n",
    "shapes_df=pd.concat([df[6],df[14],df[22],df[30],df[38],df[46],df[54],df[62],df[70],df[78]], ignore_index=True)\n",
    "stop_times_df=pd.concat([df[7],df[15],df[23],df[31],df[39],df[47],df[55],df[63],df[71],df[79]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Adding columns to existing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the orginal dataset\n",
    "final_data=dirty_data\n",
    "final_data\n",
    "final_data['Shopping_center_id']=\"Not available\"\n",
    "final_data['Distance_to_sc']=0 \n",
    "final_data['Train_station_id']=\"Not Available\"\n",
    "final_data['Distance_to_train_station']=0\n",
    "final_data['Travel_min_to_CBD']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distance function \n",
    "def distance(lat1,lat2,lon1,lon2):\n",
    "    \"\"\"    Calculate the great circle distance between two points     \n",
    "    on the earth (specified in decimal degrees)    \n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    \n",
    "    lon1,lat1,lon2,lat2 =map(radians,[lon1,lat1,lon2,lat2])\n",
    "    \n",
    "    # haversine formula \n",
    "    \n",
    "    dlon=lon2-lon1\n",
    "    dlat=lat2-lat1\n",
    "    a=sin(dlat/2)**2+cos(lat1)*cos(lat2)*sin(dlon/2)**2\n",
    "    c=2*asin(sqrt(a))\n",
    "    \n",
    "    # Radius of earth in kilometers is 6371\n",
    "    \n",
    "    km=6371*c\n",
    "    return km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Adding id and distance for Shopping centres "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to find nearest shopping centre , we first convert the latlng string in our data to two numbers \n",
    "# Then using distance function (created above), calculate the minimum distance and thus arrive at nearest shopping centre \n",
    "\n",
    "idz=[]\n",
    "distances=[]\n",
    "for i in range(len(final_data['Latlng'])):\n",
    "    string=final_data['Latlng'][i]\n",
    "    string=string.replace('[','')\n",
    "    string=string.replace(']','')\n",
    "    string=string.split(' ')\n",
    "    string=' '.join(string).split()\n",
    "    x1=float(string[0])\n",
    "    x2=float(string[1])\n",
    "    dist=[]\n",
    "    for j in range(len(shopping_centers[0]['sc_id'])):\n",
    "        # Store the distnaces for each property in the list\n",
    "        dist.append(distance(x1,shopping_centers[0]['lat'][j],x2,shopping_centers[0]['lng'][j]))\n",
    "        \n",
    "        # For the given property extract the minimum distance and the id of the Hospital \n",
    "    idz.append(shopping_centers[0]['sc_id'][dist.index(min(dist))])\n",
    "    distances.append(min(dist))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the above derived values to dataset\n",
    "\n",
    "final_data['Shopping_center_id']=idz\n",
    "final_data['Distance_to_sc']=distances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Adding id and distance for Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to find nearest station , we first convert the latlng string in our data to two numbers \n",
    "# Then using distance function (created above), calculate the minimum distance and thus arrive at nearest station\n",
    "idz2=[]\n",
    "distances2=[]\n",
    "for i in range(len(final_data['Latlng'])):\n",
    "    string=final_data['Latlng'][i]\n",
    "    string=string.replace('[','')\n",
    "    string=string.replace(']','')\n",
    "    string=string.split(' ')\n",
    "    string=' '.join(string).split()\n",
    "    x1=float(string[0])\n",
    "    x2=float(string[1])\n",
    "    dist=[]\n",
    "    for j in range(len(stops_df['stop_id'])):\n",
    "        # Store the distnaces for each property in the list\n",
    "        dist.append(distance(x1,stops_df['stop_lat'][j],x2,stops_df['stop_lon'][j]))\n",
    "        \n",
    "        # For the given property extract the minimum distance and the id of the Hospital \n",
    "    idz2.append(stops_df['stop_id'][dist.index(min(dist))])\n",
    "    distances2.append(min(dist))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the above derived values to dataset\n",
    "final_data['Train_station_id']=idz2\n",
    "final_data['Distance_to_train_station']=distances2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Adding average time to travel to CBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Airport West to Flinders Street Station (City)',\n",
       " 'City (Flinders Street)',\n",
       " 'Flinders Street',\n",
       " 'Flinders Street Station, City to Airport West',\n",
       " 'Flinders Street Station, City to North Coburg',\n",
       " 'Flinders Street Station, City to West Maribyrnong',\n",
       " 'North Coburg to Flinders Street Station (City)',\n",
       " 'West Maribyrnong to Flinders Street Station (City)'}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the trips heading to Flinders street\n",
    "x1=[]\n",
    "x2=[]\n",
    "for i in range(len(trips_df['trip_headsign'])):\n",
    "    if 'Flinders Street' in str(trips_df['trip_headsign'][i]):\n",
    "        x1.append(trips_df['trip_headsign'][i])\n",
    "set(x1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the service ids that run of weekdays \n",
    "service_id=[]\n",
    "for i in range(len(calendar_df)):\n",
    "    if calendar_df['monday'][i]==1 & calendar_df['tuesday'][i]==1 & calendar_df['wednesday'][i]==1 & calendar_df['thursday'][i]==1 & calendar_df['friday'][i]==1:\n",
    "        service_id.append(calendar_df['service_id'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### From the options we see above, only \"City (Flinders Street)\" seems right, other options are some what ambigous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_flinders_station = trips_df[trips_df['trip_headsign'] == 'City (Flinders Street)']\n",
    "\n",
    "# joining trips_flinders_station and stop_times on trip id\n",
    "cbd_trains = trips_flinders_station.set_index('trip_id').join(stop_times_df.set_index('trip_id'))\n",
    "\n",
    "# filtering the trains the run Monday to Friday\n",
    "cbd_trains = cbd_trains[cbd_trains['service_id'].isin(service_id)]\n",
    "cbd_trains.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def travel_min_to_CBD(station_id):\n",
    "    #check if are already at the destination \n",
    "    \n",
    "    if station_id['Train_station_id'] == 19854: \n",
    "        return 0\n",
    "    \n",
    "    t1 = []\n",
    "    \n",
    "    # Extract current station id\n",
    "    train_station_id = station_id['Train_station_id']\n",
    "    \n",
    "    # filerting data to Flinders street between 7-9 AM and respective station id\n",
    "    \n",
    "    station_timings = cbd_trains[((cbd_trains['stop_id'] == train_station_id)) & ((cbd_trains['arrival_time']>='07:00:00') & (cbd_trains['arrival_time']<='09:00:00'))]\n",
    "    \n",
    "    # if station id does not exists or is empty \n",
    "    if len(station_timings) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # total number station_timings obtained \n",
    "    count = len(station_timings)\n",
    "    \n",
    "    trip_id = station_timings['trip_id'].to_list()\n",
    "    for id1 in trip_id:\n",
    "        \n",
    "        # getting the time datat for station\n",
    "        tdata = cbd_trains[(cbd_trains['trip_id'] == id1) & ((cbd_trains['stop_id'] == 19854) | (cbd_trains['stop_id'] == train_station_id))]\n",
    "        tdata.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        i_station_start = tdata.loc[0,'arrival_time'] \n",
    "        \n",
    "        f_station_start = tdata.loc[1,'arrival_time'] \n",
    "        \n",
    "        # converting time strin to time format\n",
    "        i = datetime.strptime(i_station_start, '%H:%M:%S').time()\n",
    "        f = datetime.strptime(f_station_start, '%H:%M:%S').time()\n",
    "        \n",
    "        # calulting the time difference\n",
    "        time_diff = datetime.combine(date.min, f) - datetime.combine(date.min, i)\n",
    "        \n",
    "        # calculting time taken in minutes\n",
    "        time_taken = time_diff.seconds/60\n",
    "        \n",
    "        # storing the time to a list\n",
    "        t1.append(time_taken)\n",
    "    \n",
    "    # calculting average time taken in minutes\n",
    "    avg_time_taken = int(round(sum(t1)/count, 0))\n",
    "    \n",
    "    # returing the average time taken \n",
    "    return avg_time_taken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Latlng</th>\n",
       "      <th>Title</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Work_experience</th>\n",
       "      <th>Last_employment_date</th>\n",
       "      <th>Education</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Shopping_center_id</th>\n",
       "      <th>Distance_to_sc</th>\n",
       "      <th>Train_station_id</th>\n",
       "      <th>Distance_to_train_station</th>\n",
       "      <th>Travel_min_to_CBD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ID5580498</td>\n",
       "      <td>[-37.746336 144.776584]</td>\n",
       "      <td>Mr</td>\n",
       "      <td>48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>0</td>\n",
       "      <td>Finance</td>\n",
       "      <td>4452.0</td>\n",
       "      <td>SC_048</td>\n",
       "      <td>3.423812</td>\n",
       "      <td>47561</td>\n",
       "      <td>0.178246</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ID7278805</td>\n",
       "      <td>[-37.785688 145.019784]</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2019-08-30</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>3270.0</td>\n",
       "      <td>SC_001</td>\n",
       "      <td>2.765195</td>\n",
       "      <td>16844</td>\n",
       "      <td>0.297687</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ID5421840</td>\n",
       "      <td>[-37.89759827 145.0825043]</td>\n",
       "      <td>Mr</td>\n",
       "      <td>31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2019-07-17</td>\n",
       "      <td>0</td>\n",
       "      <td>Finance</td>\n",
       "      <td>2041.0</td>\n",
       "      <td>SC_042</td>\n",
       "      <td>2.386877</td>\n",
       "      <td>12936</td>\n",
       "      <td>0.220442</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ID5846738</td>\n",
       "      <td>[-37.85295486 145.286377]</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-08</td>\n",
       "      <td>0</td>\n",
       "      <td>Finance</td>\n",
       "      <td>945.0</td>\n",
       "      <td>SC_049</td>\n",
       "      <td>4.992888</td>\n",
       "      <td>16464</td>\n",
       "      <td>0.059042</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ID3240256</td>\n",
       "      <td>[-37.921072 145.165458]</td>\n",
       "      <td>Mr</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>2018-03-08</td>\n",
       "      <td>0</td>\n",
       "      <td>IT</td>\n",
       "      <td>3757.0</td>\n",
       "      <td>SC_042</td>\n",
       "      <td>8.108625</td>\n",
       "      <td>11535</td>\n",
       "      <td>0.081974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3073</th>\n",
       "      <td>1854</td>\n",
       "      <td>ID7924095</td>\n",
       "      <td>[-37.767587 145.017926]</td>\n",
       "      <td>Mr</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-08-24</td>\n",
       "      <td>0</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>SC_001</td>\n",
       "      <td>2.097952</td>\n",
       "      <td>1272</td>\n",
       "      <td>0.194176</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3074</th>\n",
       "      <td>1856</td>\n",
       "      <td>ID5661074</td>\n",
       "      <td>[-37.724984 144.769334]</td>\n",
       "      <td>Mr</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-04-11</td>\n",
       "      <td>0</td>\n",
       "      <td>Finance</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>SC_048</td>\n",
       "      <td>2.602675</td>\n",
       "      <td>47543</td>\n",
       "      <td>0.118307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3075</th>\n",
       "      <td>1876</td>\n",
       "      <td>ID5201957</td>\n",
       "      <td>[-37.840964 145.224876]</td>\n",
       "      <td>Mr</td>\n",
       "      <td>26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2018-11-20</td>\n",
       "      <td>0</td>\n",
       "      <td>Finance</td>\n",
       "      <td>2744.0</td>\n",
       "      <td>SC_032</td>\n",
       "      <td>3.155949</td>\n",
       "      <td>15173</td>\n",
       "      <td>0.235010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3076</th>\n",
       "      <td>1879</td>\n",
       "      <td>ID3142949</td>\n",
       "      <td>[-37.89935303 145.070755]</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2019-09-10</td>\n",
       "      <td>1</td>\n",
       "      <td>IT</td>\n",
       "      <td>1513.0</td>\n",
       "      <td>SC_041</td>\n",
       "      <td>1.939697</td>\n",
       "      <td>12950</td>\n",
       "      <td>0.206718</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3077</th>\n",
       "      <td>1902</td>\n",
       "      <td>ID7575471</td>\n",
       "      <td>[-37.772232 144.995411]</td>\n",
       "      <td>Mr</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>2019-12-28</td>\n",
       "      <td>1</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>5114.0</td>\n",
       "      <td>SC_001</td>\n",
       "      <td>4.104731</td>\n",
       "      <td>20016</td>\n",
       "      <td>0.263420</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3078 rows  16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0         ID                      Latlng Title  Age  Gender  \\\n",
       "0              0  ID5580498     [-37.746336 144.776584]    Mr   48     0.0   \n",
       "1              1  ID7278805     [-37.785688 145.019784]   Mrs   25     1.0   \n",
       "2              2  ID5421840  [-37.89759827 145.0825043]    Mr   31     0.0   \n",
       "3              3  ID5846738   [-37.85295486 145.286377]   Mrs   29     1.0   \n",
       "4              4  ID3240256     [-37.921072 145.165458]    Mr   47     0.0   \n",
       "...          ...        ...                         ...   ...  ...     ...   \n",
       "3073        1854  ID7924095     [-37.767587 145.017926]    Mr   29     0.0   \n",
       "3074        1856  ID5661074     [-37.724984 144.769334]    Mr   34     0.0   \n",
       "3075        1876  ID5201957     [-37.840964 145.224876]    Mr   26     0.0   \n",
       "3076        1879  ID3142949   [-37.89935303 145.070755]   Mrs   28     1.0   \n",
       "3077        1902  ID7575471     [-37.772232 144.995411]    Mr   37     0.0   \n",
       "\n",
       "      Work_experience Last_employment_date  Education        Sector  Salary  \\\n",
       "0                  13           2018-07-31          0       Finance  4452.0   \n",
       "1                   8           2019-08-30          0  Data Science  3270.0   \n",
       "2                   6           2019-07-17          0       Finance  2041.0   \n",
       "3                   2           2018-12-08          0       Finance   945.0   \n",
       "4                  13           2018-03-08          0            IT  3757.0   \n",
       "...               ...                  ...        ...           ...     ...   \n",
       "3073                6           2018-08-24          0  Data Science  2250.0   \n",
       "3074                5           2018-04-11          0       Finance  1680.0   \n",
       "3075                8           2018-11-20          0       Finance  2744.0   \n",
       "3076                3           2019-09-10          1            IT  1513.0   \n",
       "3077               11           2019-12-28          1  Data Science  5114.0   \n",
       "\n",
       "     Shopping_center_id  Distance_to_sc  Train_station_id  \\\n",
       "0                SC_048        3.423812             47561   \n",
       "1                SC_001        2.765195             16844   \n",
       "2                SC_042        2.386877             12936   \n",
       "3                SC_049        4.992888             16464   \n",
       "4                SC_042        8.108625             11535   \n",
       "...                 ...             ...               ...   \n",
       "3073             SC_001        2.097952              1272   \n",
       "3074             SC_048        2.602675             47543   \n",
       "3075             SC_032        3.155949             15173   \n",
       "3076             SC_041        1.939697             12950   \n",
       "3077             SC_001        4.104731             20016   \n",
       "\n",
       "      Distance_to_train_station  Travel_min_to_CBD  \n",
       "0                      0.178246                  0  \n",
       "1                      0.297687                  0  \n",
       "2                      0.220442                  0  \n",
       "3                      0.059042                  0  \n",
       "4                      0.081974                  0  \n",
       "...                         ...                ...  \n",
       "3073                   0.194176                  0  \n",
       "3074                   0.118307                  0  \n",
       "3075                   0.235010                  0  \n",
       "3076                   0.206718                  0  \n",
       "3077                   0.263420                 18  \n",
       "\n",
       "[3078 rows x 16 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the final column completing the resultant dataframe \n",
    "# Applying the above created travel_min_to_CBD function to find averge time taken to travel to cbd for each station id \n",
    "final_data['Travel_min_to_CBD'] = final_data.apply(travel_min_to_CBD,axis=1,result_type='expand')\n",
    "final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Save the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv(\"dirty_data_solution.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following conclusions are derived after performing the above two tasks\n",
    "#### 1) Task 1 was executed and the derive data was saved in .csv format\n",
    "#### 2) Task 2 was executed and the derive data was saved in .json and .txt format \n",
    "#### 3) Task 3 was executed and the derive data was saved in .csv format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Blacklight., (8 April 2013).How to solve ValurError Math Domain.https://stackoverflow.com/questions/15890503/valueerror-math-domain-error/15890593\n",
    "\n",
    "* Rudolf Morkovskyi (19 October 2010 ) How to calculate distance using euclidean distance for latitude and logitude dataframe pyhon . https://stackoverflow.com/questions/52889566/calculate-euclidean-distance-for-latitude-and-longitude-pandas-dataframe-pytho\n",
    "\n",
    "* Sarthak Girdhar  (7 September 2015) . Read XML file to python dataframe .https://stackoverflow.com/questions/52968877/read-xml-file-to-pandas-dataframe\n",
    "\n",
    "* Arun_2089   (16 may 2014) . Key error 0 when accessing value in pandas series https://stackoverflow.com/questions/46153647/keyerror-0-when-accessing-value-in-pandas-series   \n",
    "\n",
    "* Parfeit (17 Jan 2018) Looped regression model in Python/Sklearn https://stackoverflow.com/questions/51388624/looped-regression-model-in-python-sklearn\n",
    "\n",
    "* Wes McKinney (20 Jun 2012 ) Adding two pandas Dataframes https://stackoverflow.com/questions/11106823/adding-two-pandas-dataframes\n",
    "\n",
    "* fusionfan (26 May 2016) Read the data from TXT file inside Zip File without extracting the contents in Python https://stackoverflow.com/questions/37456361/read-the-data-from-txt-file-inside-zip-file-without-extracting-the-contents-in-m \n",
    "\n",
    "* Steven Hunter Douglas (16 Sep 2015) How to loop sklearn linear regression by value within column python https://stackoverflow.com/questions/32615280/how-to-loop-sklearn-linear-regression-by-values-within-a-column-python \n",
    "\n",
    "* wilbev (16 May 2012) Convert datetime object to a String of date only in Python https://stackoverflow.com/questions/10624937/convert-datetime-object-to-a-string-of-date-only-in-python\n",
    "\n",
    "* EdChum (13 Jun 2016) how to sort pandas dataframe from one column https://stackoverflow.com/questions/37787698/how-to-sort-pandas-dataframe-from-one-column\n",
    "\n",
    "* jezrael (7 Mar 2017) Convert list into a pandas data frame https://stackoverflow.com/questions/42593104/convert-list-into-a-pandas-data-frame"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
